{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import mlflow\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import plotly.graph_objects as go\n",
    "from huggingface_hub import login\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import calculate_metrics, TrainingConfig\n",
    "\n",
    "dotenv.load_dotenv(\"../../.env\")\n",
    "\n",
    "token = os.environ[\"HF_TOKEN\"]\n",
    "login(token=token)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Time_Series_Forecasting\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/panama-electricity-load-forecasting/processed/'\n",
    "\n",
    "train_df = pd.read_parquet(os.path.join(data_dir, 'train.parquet'))\n",
    "test_df = pd.read_parquet(os.path.join(data_dir, 'test.parquet'))\n",
    "\n",
    "test_len = test_df['datetime'].max() - test_df['datetime'].min()\n",
    "# val_df = train_df[train_df['datetime'] >= train_df['datetime'].max() - test_len]\n",
    "# train_df = train_df[train_df['datetime'] < train_df['datetime'].max() - test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_covariates_names = [\n",
    "    \"T2M_toc\",\n",
    "    \"QV2M_toc\",\n",
    "    \"TQL_toc\",\n",
    "    \"W2M_toc\",\n",
    "    \"T2M_san\",\n",
    "    \"QV2M_san\",\n",
    "    \"TQL_san\",\n",
    "    \"W2M_san\",\n",
    "    \"T2M_dav\",\n",
    "    \"QV2M_dav\",\n",
    "    \"TQL_dav\",\n",
    "    \"W2M_dav\",\n",
    "]\n",
    "\n",
    "known_covariates_names = [\n",
    "    \"holiday\",\n",
    "    \"school\",\n",
    "]\n",
    "\n",
    "train_df = (\n",
    "    train_df[[\"datetime\", \"nat_demand\"] + past_covariates_names + known_covariates_names]\n",
    "    .rename(columns={\"datetime\": \"timestamp\", \"nat_demand\": \"target\"})\n",
    "    .copy()\n",
    ")\n",
    "train_df[\"item_id\"] = 0\n",
    "\n",
    "# val_df = val_df[['datetime', 'nat_demand'] + known_covariates_names].rename(columns={'datetime': 'timestamp', 'nat_demand': 'target'}).copy()\n",
    "# val_df['item_id'] = 0\n",
    "\n",
    "test_df = (\n",
    "    test_df[[\"datetime\", \"nat_demand\"] + past_covariates_names + known_covariates_names]\n",
    "    .rename(columns={\"datetime\": \"timestamp\", \"nat_demand\": \"target\"})\n",
    "    .copy()\n",
    ")\n",
    "test_df[\"item_id\"] = 0\n",
    "\n",
    "train_data = TimeSeriesDataFrame.from_data_frame(train_df, id_column='item_id', timestamp_column='timestamp')\n",
    "# val_data = TimeSeriesDataFrame.from_data_frame(val_df, id_column='item_id', timestamp_column='timestamp')\n",
    "test_data = TimeSeriesDataFrame.from_data_frame(test_df, id_column='item_id', timestamp_column='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig(\n",
    "    prediction_length=24 * 3,  # 3 дня\n",
    "    artifact_path=\"../../models/auto_ml_all_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../../models/auto_ml_all_data\"\n",
      "Beginning AutoGluon training...\n",
      "AutoGluon will save models to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2\n",
      "CPU Count:          12\n",
      "GPU Count:          1\n",
      "Memory Avail:       19.40 GB / 30.95 GB (62.7%)\n",
      "Disk Space Avail:   161.14 GB / 233.67 GB (69.0%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': False,\n",
      " 'eval_metric': WQL,\n",
      " 'hyperparameters': {'Chronos': [{'ag_args': {'name_suffix': 'ZeroShot'},\n",
      "                                  'model_path': 'bolt_small'},\n",
      "                                 {'ag_args': {'name_suffix': 'FineTuned'},\n",
      "                                  'fine_tune': True,\n",
      "                                  'model_path': 'bolt_small'}],\n",
      "                     'DirectTabular': {},\n",
      "                     'RecursiveTabular': {},\n",
      "                     'TemporalFusionTransformer': {}},\n",
      " 'known_covariates_names': ['holiday', 'school'],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 72,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'verbosity': 4}\n",
      "\n",
      "Inferred time series frequency: 'h'\n",
      "Provided train_data has 43775 rows, 1 time series. Median time series length is 43775 (min=43775, max=43775). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\tknown_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['holiday', 'school']\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['T2M_toc', 'QV2M_toc', 'TQL_toc', 'W2M_toc', 'T2M_san', 'QV2M_san', ...]\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-04-22 13:04:51\n",
      "Models that will be trained: ['RecursiveTabular', 'DirectTabular', 'ChronosZeroShot[bolt_small]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer']\n",
      "Training timeseries model RecursiveTabular. \n",
      "\tWindow 0\n",
      "Shortening all series to at most 1000096\n",
      "train_df shape: (43607, 49), val_df shape: (72, 49)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2\n",
      "CPU Count:          12\n",
      "Memory Avail:       19.35 GB / 30.95 GB (62.5%)\n",
      "Disk Space Avail:   161.14 GB / 233.67 GB (69.0%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/RecursiveTabular/W0/tabular_predictor\"\n",
      "Train Data Rows:    43607\n",
      "Train Data Columns: 46\n",
      "Tuning Data Rows:    72\n",
      "Tuning Data Columns: 46\n",
      "Label Column:       y\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19824.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.66 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 46 | ['holiday', 'school', 'lag1', 'lag2', 'lag3', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 44 | ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['holiday', 'school']\n",
      "\t0.1s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.41 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.151291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.1512\t = Validation score   (-mean_absolute_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t-0.1512\t = Validation score   (-mean_absolute_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.12s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 51888.3 rows/s (72 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/RecursiveTabular/W0/tabular_predictor\")\n",
      "Shortening all series to at most 1000096\n",
      "\t\t-0.0356      = Validation score (-WQL)\n",
      "\t\t2.245   s    = Training runtime\n",
      "\t\t0.682   s    = Prediction runtime\n",
      "\t-0.0356       = Validation score (-WQL)\n",
      "\t2.26    s     = Training runtime\n",
      "\t0.68    s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. \n",
      "\tWindow 0\n",
      "Shortening all series to at most 1000072\n",
      "train_df shape: (43631, 49), val_df shape: (72, 49)\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2\n",
      "CPU Count:          12\n",
      "Memory Avail:       19.31 GB / 30.95 GB (62.4%)\n",
      "Disk Space Avail:   161.13 GB / 233.67 GB (69.0%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/DirectTabular/W0/tabular_predictor\"\n",
      "Train Data Rows:    43631\n",
      "Train Data Columns: 46\n",
      "Tuning Data Rows:    72\n",
      "Tuning Data Columns: 46\n",
      "Label Column:       y\n",
      "Problem Type:       quantile\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19760.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.67 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 46 | ['holiday', 'school', 'lag1', 'lag2', 'lag3', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 44 | ['lag1', 'lag2', 'lag3', 'lag4', 'lag5', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['holiday', 'school']\n",
      "\t0.1s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.42 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'pinball_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM ...\n",
      "\t-0.0131\t = Validation score   (-pinball_loss)\n",
      "\t24.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBM': 1.0}\n",
      "\t-0.0131\t = Validation score   (-pinball_loss)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 24.83s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8479.5 rows/s (72 batch size)\n",
      "Disabling calibration for metric `pinball_loss` due to having fewer than 1000 rows of validation data for calibration, to avoid overfitting (72 rows). Force calibration via specifying `calibrate=True`. (calibrate='auto')\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/DirectTabular/W0/tabular_predictor\")\n",
      "Shortening all series to at most 1000144\n",
      "Shortening all series to at most 1000072\n",
      "\t\t-0.0222      = Validation score (-WQL)\n",
      "\t\t24.951  s    = Training runtime\n",
      "\t\t0.219   s    = Prediction runtime\n",
      "\t-0.0222       = Validation score (-WQL)\n",
      "\t24.96   s     = Training runtime\n",
      "\t0.22    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_small]. \n",
      "\tWindow 0\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "\t\t-0.0460      = Validation score (-WQL)\n",
      "\t\t0.812   s    = Training runtime\n",
      "\t\t1.524   s    = Prediction runtime\n",
      "\t-0.0460       = Validation score (-WQL)\n",
      "\t0.82    s     = Training runtime\n",
      "\t1.52    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. \n",
      "\tWindow 0\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\tChronosBolt models can only be fine-tuned with a maximum prediction_length of 64. Fine-tuning prediction_length has been changed to 64.\n",
      "PyTorch: setting up devices\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Transformers logging is turned on during fine-tuning. Note that losses reported by transformers may not correspond to those specified via `eval_metric`.\n",
      "***** Running training *****\n",
      "  Num examples = 32,000\n",
      "  Num Epochs = 9,223,372,036,854,775,807\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 47,718,016\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 11.3937, 'grad_norm': 29.075719833374023, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 11.051, 'grad_norm': 35.23099899291992, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 10.9488, 'grad_norm': 40.49997329711914, 'learning_rate': 7e-06, 'epoch': 0.3}\n",
      "{'loss': 10.8414, 'grad_norm': 25.587913513183594, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 30.6875, 'grad_norm': 32.58864212036133, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 10.7451, 'grad_norm': 38.01239013671875, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 10.7977, 'grad_norm': 30.45256996154785, 'learning_rate': 3e-06, 'epoch': 0.7}\n",
      "{'loss': 10.671, 'grad_norm': 40.5295524597168, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n",
      "{'loss': 10.3869, 'grad_norm': 30.922677993774414, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.9}\n",
      "{'loss': 10.8314, 'grad_norm': 35.8390007019043, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 32\n",
      "{'eval_loss': 20.86516761779785, 'eval_runtime': 0.0076, 'eval_samples_per_second': 131.479, 'eval_steps_per_second': 131.479, 'epoch': 1.0}\n",
      "Saving model checkpoint to /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/transformers_logs/checkpoint-1000\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/transformers_logs/checkpoint-1000/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/transformers_logs/checkpoint-1000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 129.2503, 'train_samples_per_second': 247.582, 'train_steps_per_second': 7.737, 'train_loss': 12.835447143554687, 'epoch': 1.0}\n",
      "\tSaving fine-tuned model to /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/fine-tuned-ckpt\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/fine-tuned-ckpt/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/fine-tuned-ckpt/model.safetensors\n",
      "Removing transformers_logs directory /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/ChronosFineTuned[bolt_small]/W0/transformers_logs\n",
      "\t\t-0.0490      = Validation score (-WQL)\n",
      "\t\t130.804 s    = Training runtime\n",
      "\t\t0.015   s    = Prediction runtime\n",
      "\t-0.0490       = Validation score (-WQL)\n",
      "\t130.82  s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model TemporalFusionTransformer. \n",
      "\tWindow 0\n",
      "\tbool_features: ['holiday', 'school'], continuous_features: [], skewed_features: []\n",
      "\tbool_features: [], continuous_features: ['T2M_toc', 'QV2M_toc', 'W2M_toc', 'T2M_san', 'QV2M_san', 'TQL_san', 'T2M_dav', 'QV2M_dav', 'TQL_dav', 'W2M_dav'], skewed_features: ['TQL_toc', 'W2M_san']\n",
      "GluonTS logging is turned on during training. Note that losses reported by GluonTS may not correspond to those specified via `eval_metric`.\n",
      "\tTraining on device 'gpu'\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                           | Params | Mode  | In sizes                                                                                  | Out sizes                     \n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TemporalFusionTransformerModel | 138 K  | train | [[1, 144], [1, 144], [1, 1], [1, 1], [1, 216, 6], [1, 216, 0], [1, 144, 12], [1, 144, 0]] | [[[1, 72, 9]], [1, 1], [1, 1]]\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.555     Total estimated model params size (MB)\n",
      "256       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 0, global step 50: 'val_loss' reached 57.40743 (best 57.40743), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'val_loss' reached 43.91767 (best 43.91767), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'val_loss' reached 41.77654 (best 41.77654), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=2-step=150.ckpt' as top 1\n",
      "Epoch 3, global step 200: 'val_loss' reached 40.70953 (best 40.70953), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=3-step=200.ckpt' as top 1\n",
      "Epoch 4, global step 250: 'val_loss' reached 34.29255 (best 34.29255), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=4-step=250.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'val_loss' reached 26.39864 (best 26.39864), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=5-step=300.ckpt' as top 1\n",
      "Epoch 6, global step 350: 'val_loss' was not in top 1\n",
      "Epoch 7, global step 400: 'val_loss' was not in top 1\n",
      "Epoch 8, global step 450: 'val_loss' was not in top 1\n",
      "Epoch 9, global step 500: 'val_loss' was not in top 1\n",
      "Epoch 10, global step 550: 'val_loss' reached 24.96748 (best 24.96748), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=10-step=550.ckpt' as top 1\n",
      "Epoch 11, global step 600: 'val_loss' was not in top 1\n",
      "Epoch 12, global step 650: 'val_loss' was not in top 1\n",
      "Epoch 13, global step 700: 'val_loss' was not in top 1\n",
      "Epoch 14, global step 750: 'val_loss' was not in top 1\n",
      "Epoch 15, global step 800: 'val_loss' was not in top 1\n",
      "Epoch 16, global step 850: 'val_loss' reached 23.21929 (best 23.21929), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=16-step=850.ckpt' as top 1\n",
      "Epoch 17, global step 900: 'val_loss' reached 22.58281 (best 22.58281), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=17-step=900.ckpt' as top 1\n",
      "Epoch 18, global step 950: 'val_loss' was not in top 1\n",
      "Epoch 19, global step 1000: 'val_loss' was not in top 1\n",
      "Epoch 20, global step 1050: 'val_loss' reached 21.58999 (best 21.58999), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=20-step=1050.ckpt' as top 1\n",
      "Epoch 21, global step 1100: 'val_loss' was not in top 1\n",
      "Epoch 22, global step 1150: 'val_loss' was not in top 1\n",
      "Epoch 23, global step 1200: 'val_loss' was not in top 1\n",
      "Epoch 24, global step 1250: 'val_loss' was not in top 1\n",
      "Epoch 25, global step 1300: 'val_loss' was not in top 1\n",
      "Epoch 26, global step 1350: 'val_loss' was not in top 1\n",
      "Epoch 27, global step 1400: 'val_loss' reached 19.40967 (best 19.40967), saving model to '/home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs/version_0/checkpoints/epoch=27-step=1400.ckpt' as top 1\n",
      "Epoch 28, global step 1450: 'val_loss' was not in top 1\n",
      "Epoch 29, global step 1500: 'val_loss' was not in top 1\n",
      "Epoch 30, global step 1550: 'val_loss' was not in top 1\n",
      "Epoch 31, global step 1600: 'val_loss' was not in top 1\n",
      "Epoch 32, global step 1650: 'val_loss' was not in top 1\n",
      "Epoch 33, global step 1700: 'val_loss' was not in top 1\n",
      "Epoch 34, global step 1750: 'val_loss' was not in top 1\n",
      "Epoch 35, global step 1800: 'val_loss' was not in top 1\n",
      "Epoch 36, global step 1850: 'val_loss' was not in top 1\n",
      "Epoch 37, global step 1900: 'val_loss' was not in top 1\n",
      "Epoch 38, global step 1950: 'val_loss' was not in top 1\n",
      "Epoch 39, global step 2000: 'val_loss' was not in top 1\n",
      "Epoch 40, global step 2050: 'val_loss' was not in top 1\n",
      "Epoch 41, global step 2100: 'val_loss' was not in top 1\n",
      "Epoch 42, global step 2150: 'val_loss' was not in top 1\n",
      "Epoch 43, global step 2200: 'val_loss' was not in top 1\n",
      "Epoch 44, global step 2250: 'val_loss' was not in top 1\n",
      "Epoch 45, global step 2300: 'val_loss' was not in top 1\n",
      "Epoch 46, global step 2350: 'val_loss' was not in top 1\n",
      "Epoch 47, global step 2400: 'val_loss' was not in top 1\n",
      "Removing lightning_logs directory /home/nikita/projects/time_series_analysis/models/auto_ml_all_data/models/TemporalFusionTransformer/W0/lightning_logs\n",
      "\tWarning: Exception caused TemporalFusionTransformer to fail during training... Skipping this model.\n",
      "\tmodule 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/trainer/abstract_trainer.py\", line 525, in _train_and_save\n",
      "    model = self._train_single(train_data, model, val_data=val_data, time_limit=time_limit)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/trainer/abstract_trainer.py\", line 441, in _train_single\n",
      "    model.fit(\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 307, in fit\n",
      "    return super().fit(train_data=train_data, val_data=val_data, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/models/multi_window/multi_window_model.py\", line 147, in _fit\n",
      "    model.score_and_cache_oof(\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 528, in score_and_cache_oof\n",
      "    oof_predictions = self.predict(past_data, known_covariates=known_covariates, **predict_kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 432, in predict\n",
      "    predictions = self._predict(data=data, known_covariates=known_covariates, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/models/gluonts/abstract_gluonts.py\", line 530, in _predict\n",
      "    predicted_targets = self._predict_gluonts_forecasts(data, known_covariates=known_covariates, **kwargs)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/models/gluonts/abstract_gluonts.py\", line 545, in _predict_gluonts_forecasts\n",
      "    return list(self.gts_predictor.predict(**predictor_kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/gluonts/torch/model/predictor.py\", line 90, in predict\n",
      "    yield from self.forecast_generator(\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/gluonts/model/forecast_generator.py\", line 130, in __call__\n",
      "    (outputs,), loc, scale = make_predictions(prediction_net, inputs)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/gluonts/model/forecast_generator.py\", line 88, in make_predictions\n",
      "    import mxnet as mx\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/__init__.py\", line 33, in <module>\n",
      "    from . import contrib\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/contrib/__init__.py\", line 30, in <module>\n",
      "    from . import text\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/contrib/text/__init__.py\", line 23, in <module>\n",
      "    from . import embedding\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/contrib/text/embedding.py\", line 36, in <module>\n",
      "    from ... import numpy as _mx_np\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/numpy/__init__.py\", line 23, in <module>\n",
      "    from .multiarray import *  # pylint: disable=wildcard-import\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/numpy/multiarray.py\", line 47, in <module>\n",
      "    from .utils import _get_np_op\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/mxnet/numpy/utils.py\", line 37, in <module>\n",
      "    bool = onp.bool\n",
      "           ^^^^^^^^\n",
      "  File \"/home/nikita/anaconda3/lib/python3.12/site-packages/numpy/__init__.py\", line 324, in __getattr__\n",
      "    raise AttributeError(__former_attrs__[attr])\n",
      "AttributeError: module 'numpy' has no attribute 'bool'.\n",
      "`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'bool_'?\n",
      "\n",
      "Training complete. Models trained: ['RecursiveTabular', 'DirectTabular', 'ChronosZeroShot[bolt_small]', 'ChronosFineTuned[bolt_small]']\n",
      "Total runtime: 232.12 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.0222\n"
     ]
    }
   ],
   "source": [
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=config.prediction_length,\n",
    "    path=config.artifact_path,\n",
    "    known_covariates_names=known_covariates_names,\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    verbosity=4,\n",
    "    hyperparameters={\n",
    "        \"DirectTabular\": {},\n",
    "        \"RecursiveTabular\": {},\n",
    "        \"TemporalFusionTransformer\": {},\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\"model_path\": \"bolt_small\", \"fine_tune\": True, \"ag_args\": {\"name_suffix\": \"FineTuned\"}},\n",
    "        ]\n",
    "    },\n",
    "    enable_ensemble=False,\n",
    "    #presets=\"high_quality\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TimeSeriesPredictor.load(config.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "WQL_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WQL_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fit_time_marginal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fit_order",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MASE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAPE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SQL",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8eefbde0-0b4f-45de-870d-b60dbb71bf0d",
       "rows": [
        [
         "0",
         "ChronosFineTuned[bolt_small]",
         "-0.014153820687496679",
         "-0.049029416962903596",
         "0.11309051513671875",
         "0.014652013778686523",
         "130.81596040725708",
         "4",
         "-0.21861635393991136",
         "-0.016884335856077803",
         "-646.7964972103496",
         "-20.63585285644531",
         "-0.19047690668672526"
        ],
        [
         "1",
         "ChronosZeroShot[bolt_small]",
         "-0.015607458587219838",
         "-0.04598145252375115",
         "1.3568766117095947",
         "1.5242130756378174",
         "0.823530912399292",
         "3",
         "-0.23661722673256425",
         "-0.018131986824131842",
         "-780.5194749860942",
         "-22.335009189181854",
         "-0.21003943024098026"
        ],
        [
         "2",
         "RecursiveTabular",
         "-0.03293755688913883",
         "-0.035576081828809475",
         "0.6023671627044678",
         "0.6819674968719482",
         "2.257936716079712",
         "1",
         "-0.4036835902088105",
         "-0.031578897960069345",
         "-1830.629494346327",
         "-38.10490394694009",
         "-0.44326151140260267"
        ],
        [
         "3",
         "DirectTabular",
         "-0.040739939861101304",
         "-0.022180429870208584",
         "0.1136617660522461",
         "0.2191002368927002",
         "24.96207594871521",
         "2",
         "-0.6953352172104835",
         "-0.054586101868198705",
         "-4968.161087190636",
         "-65.63477511935764",
         "-0.5482631082221426"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>WQL_test</th>\n",
       "      <th>WQL_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>fit_order</th>\n",
       "      <th>MASE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>SQL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChronosFineTuned[bolt_small]</td>\n",
       "      <td>-0.014154</td>\n",
       "      <td>-0.049029</td>\n",
       "      <td>0.113091</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>130.815960</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.218616</td>\n",
       "      <td>-0.016884</td>\n",
       "      <td>-646.796497</td>\n",
       "      <td>-20.635853</td>\n",
       "      <td>-0.190477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChronosZeroShot[bolt_small]</td>\n",
       "      <td>-0.015607</td>\n",
       "      <td>-0.045981</td>\n",
       "      <td>1.356877</td>\n",
       "      <td>1.524213</td>\n",
       "      <td>0.823531</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.236617</td>\n",
       "      <td>-0.018132</td>\n",
       "      <td>-780.519475</td>\n",
       "      <td>-22.335009</td>\n",
       "      <td>-0.210039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RecursiveTabular</td>\n",
       "      <td>-0.032938</td>\n",
       "      <td>-0.035576</td>\n",
       "      <td>0.602367</td>\n",
       "      <td>0.681967</td>\n",
       "      <td>2.257937</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.403684</td>\n",
       "      <td>-0.031579</td>\n",
       "      <td>-1830.629494</td>\n",
       "      <td>-38.104904</td>\n",
       "      <td>-0.443262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DirectTabular</td>\n",
       "      <td>-0.040740</td>\n",
       "      <td>-0.022180</td>\n",
       "      <td>0.113662</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>24.962076</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.695335</td>\n",
       "      <td>-0.054586</td>\n",
       "      <td>-4968.161087</td>\n",
       "      <td>-65.634775</td>\n",
       "      <td>-0.548263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  WQL_test   WQL_val  pred_time_test  \\\n",
       "0  ChronosFineTuned[bolt_small] -0.014154 -0.049029        0.113091   \n",
       "1   ChronosZeroShot[bolt_small] -0.015607 -0.045981        1.356877   \n",
       "2              RecursiveTabular -0.032938 -0.035576        0.602367   \n",
       "3                 DirectTabular -0.040740 -0.022180        0.113662   \n",
       "\n",
       "   pred_time_val  fit_time_marginal  fit_order      MASE      MAPE  \\\n",
       "0       0.014652         130.815960          4 -0.218616 -0.016884   \n",
       "1       1.524213           0.823531          3 -0.236617 -0.018132   \n",
       "2       0.681967           2.257937          1 -0.403684 -0.031579   \n",
       "3       0.219100          24.962076          2 -0.695335 -0.054586   \n",
       "\n",
       "           MSE        MAE       SQL  \n",
       "0  -646.796497 -20.635853 -0.190477  \n",
       "1  -780.519475 -22.335009 -0.210039  \n",
       "2 -1830.629494 -38.104904 -0.443262  \n",
       "3 -4968.161087 -65.634775 -0.548263  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(\n",
    "    test_data,\n",
    "    extra_metrics=['MASE', 'MAPE', 'MSE', 'MAE', 'SQL'],\n",
    ")\n",
    "leaderboard.rename(columns={'score_test': 'WQL_test', 'score_val': 'WQL_val'}, inplace=True)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/22 13:12:14 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run ChronosFineTuned[bolt_small]_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/505308ffa2024e3f89acdb29d8c41454\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n",
      "🏃 View run ChronosZeroShot[bolt_small]_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/7468135871e543de9c01dc4556f093b7\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n",
      "🏃 View run RecursiveTabular_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/e4f1d0ab06ed4b419e8007babe6e14d2\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n",
      "🏃 View run DirectTabular_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/be7abd0fdb59477693dbd2ec72c0d071\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n"
     ]
    }
   ],
   "source": [
    "from utils.mlflow_logging import log_leaderboard_to_mlflow\n",
    "\n",
    "log_leaderboard_to_mlflow(leaderboard, 'AllData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 12\n",
    "\n",
    "top_k_models = leaderboard.sort_values(['SQL'], ascending=False).head(k)['model'].tolist()\n",
    "window_size = config.prediction_length\n",
    "test_length = len(test_df)\n",
    "max_iterations = (test_length + window_size - 1) // window_size - 1 # ещё -1 из-за known_covariates\n",
    "\n",
    "current_data = train_data.copy()\n",
    "all_models_predictions = {}\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    for model_name in top_k_models:\n",
    "        if model_name not in all_models_predictions:\n",
    "            all_models_predictions[model_name] = []\n",
    "            \n",
    "        start_idx = i * window_size\n",
    "        end_idx = start_idx + window_size\n",
    "        \n",
    "        future_covariates = test_data[start_idx:start_idx + config.prediction_length][known_covariates_names]\n",
    "        prediction_covariates = pd.concat([current_data[known_covariates_names], future_covariates])\n",
    "        \n",
    "        predictions = predictor.predict(current_data, \n",
    "                                       model=model_name, \n",
    "                                       known_covariates=prediction_covariates)\n",
    "                                       \n",
    "        all_models_predictions[model_name].append(predictions)\n",
    "        \n",
    "    current_data = pd.concat([current_data, test_data[start_idx:end_idx]])\n",
    "\n",
    "test_df_shape = test_df.shape[0]\n",
    "all_models_predictions = {k: pd.concat(v)[:test_df_shape] for k, v in all_models_predictions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d56507a33b43d6ad955c1c74da0200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(DatePicker(value=datetime.date(2020, 1, 1), description='Start date:'), DatePick…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9cce31f85a4305a95e2eaa64ea37e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import datetime\n",
    "from utils.plotting import plot_forecasts\n",
    "\n",
    "min_date = test_df[\"timestamp\"].min().date()\n",
    "max_date = test_df[\"timestamp\"].max().date()\n",
    "\n",
    "start_date_picker = widgets.DatePicker(\n",
    "    description='Start date:',\n",
    "    disabled=False,\n",
    "    value=min_date\n",
    ")\n",
    "\n",
    "end_date_picker = widgets.DatePicker(\n",
    "    description='End date:',\n",
    "    disabled=False,\n",
    "    value=max_date\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        start_date = datetime.datetime.combine(start_date_picker.value, datetime.datetime.min.time())\n",
    "        end_date = datetime.datetime.combine(end_date_picker.value, datetime.datetime.min.time())\n",
    "        plot_forecasts(df=test_df[:config.prediction_length * max_iterations],models_predictions=all_models_predictions, start_date=start_date, end_date=end_date)\n",
    "\n",
    "plot_button = widgets.Button(description=\"Plot Forecasts\")\n",
    "plot_button.on_click(on_button_clicked)\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HBox([start_date_picker, end_date_picker]),\n",
    "    plot_button\n",
    "])\n",
    "\n",
    "display(controls, output_area)\n",
    "\n",
    "with output_area:\n",
    "    plot_forecasts(df=test_df[:config.prediction_length * max_iterations],models_predictions=all_models_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5488.754710196724"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_df['target']\n",
    "y_pred = all_models_predictions['ChronosFineTuned[bolt_small]']['mean'].values\n",
    "\n",
    "sk_m.mean_squared_error(y_true=y_true[:y_pred.shape[0]], y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-01-31 23:00:00')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['timestamp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "T2M_toc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "QV2M_toc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TQL_toc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "W2M_toc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "T2M_san",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "QV2M_san",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TQL_san",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "W2M_san",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "T2M_dav",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "QV2M_dav",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TQL_dav",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "W2M_dav",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "holiday",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "school",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "item_id",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "94d26840-b923-4c83-bad1-8adfe3906ea0",
       "rows": [
        [
         "216",
         "2020-01-10 00:00:00",
         "1131.5281",
         "24.9298645",
         "0.01728508",
         "1.26e-05",
         "21.30186785",
         "23.750177",
         "0.016995164",
         "0.003576279",
         "10.69691807",
         "22.984552",
         "0.016171189",
         "0.07513428",
         "6.629415237",
         "0",
         "0",
         "0"
        ],
        [
         "217",
         "2020-01-10 01:00:00",
         "1093.7796",
         "24.92293701",
         "0.017269915",
         "0.001275063",
         "21.95450025",
         "23.50106201",
         "0.016682452",
         "0.004701614",
         "11.57964691",
         "22.81356201",
         "0.015919512",
         "0.07223511",
         "6.986607498",
         "0",
         "0",
         "0"
        ],
        [
         "218",
         "2020-01-10 02:00:00",
         "1061.8127",
         "24.9710022",
         "0.017232565",
         "0.001014233",
         "22.54558444",
         "23.1975647",
         "0.016317038",
         "0.002249718",
         "12.12610565",
         "22.5960022",
         "0.015641836",
         "0.06549072",
         "7.229447086",
         "0",
         "0",
         "0"
        ],
        [
         "219",
         "2020-01-10 03:00:00",
         "1037.688",
         "25.02651367",
         "0.017226303",
         "0.001494408",
         "23.02676729",
         "22.94057617",
         "0.015959824",
         "0.002109528",
         "12.27851698",
         "22.37026367",
         "0.015418137",
         "0.055236816",
         "7.265826059",
         "0",
         "0",
         "0"
        ],
        [
         "220",
         "2020-01-10 04:00:00",
         "1038.856",
         "25.14516602",
         "0.017372904",
         "0.005374908",
         "23.50984946",
         "22.69204102",
         "0.015526592",
         "0.002664566",
         "12.06539349",
         "22.15297852",
         "0.015206157",
         "0.04890442",
         "7.164662619",
         "0",
         "0",
         "0"
        ],
        [
         "221",
         "2020-01-10 05:00:00",
         "1055.3087",
         "25.26976929",
         "0.017671112",
         "0.010196686",
         "22.56727418",
         "22.40258179",
         "0.015149598",
         "0.002672195",
         "11.45702435",
         "21.95726929",
         "0.014970307",
         "0.044326782",
         "6.937178692",
         "0",
         "0",
         "0"
        ],
        [
         "222",
         "2020-01-10 06:00:00",
         "1082.6026",
         "25.42458496",
         "0.01793735",
         "0.010807037",
         "21.02945435",
         "22.01052246",
         "0.014992403",
         "0.000932932",
         "10.06387371",
         "21.83083496",
         "0.014813112",
         "0.036224365",
         "6.815983872",
         "0",
         "0",
         "0"
        ],
        [
         "223",
         "2020-01-10 07:00:00",
         "1189.3739",
         "26.34261475",
         "0.018401418",
         "0.004987717",
         "22.47867092",
         "23.38167725",
         "0.01552895",
         "0.000407219",
         "12.76278647",
         "22.17855225",
         "0.014781269",
         "0.028297424",
         "7.253475635",
         "0",
         "0",
         "0"
        ],
        [
         "224",
         "2020-01-10 08:00:00",
         "1358.5646",
         "27.56295166",
         "0.018544598",
         "0.006883621",
         "25.58716218",
         "25.21920166",
         "0.01637785",
         "0.001126289",
         "14.86867916",
         "23.14888916",
         "0.014752788",
         "0.02671814",
         "7.423430537",
         "0",
         "0",
         "0"
        ],
        [
         "225",
         "2020-01-10 09:00:00",
         "1441.442",
         "28.57503662",
         "0.018543037",
         "0.014431",
         "27.27388341",
         "27.11409912",
         "0.016811164",
         "0.003285408",
         "17.3411545",
         "24.30941162",
         "0.014640601",
         "0.030761719",
         "7.074664455",
         "0",
         "0",
         "0"
        ],
        [
         "226",
         "2020-01-10 10:00:00",
         "1535.9506",
         "29.35112915",
         "0.018335853",
         "0.022994995",
         "29.08063799",
         "28.41362915",
         "0.017077003",
         "0.007915497",
         "17.31653132",
         "25.36675415",
         "0.014563117",
         "0.03163147",
         "6.769717296",
         "0",
         "0",
         "0"
        ],
        [
         "227",
         "2020-01-10 11:00:00",
         "1561.8585",
         "29.90993652",
         "0.01806052",
         "0.018081665",
         "29.85483985",
         "29.40212402",
         "0.017541721",
         "0.01890564",
         "16.87115728",
         "26.22243652",
         "0.014589145",
         "0.039627075",
         "6.482674514",
         "0",
         "0",
         "0"
        ],
        [
         "228",
         "2020-01-10 12:00:00",
         "1577.0431",
         "30.22515259",
         "0.017908983",
         "0.011486054",
         "29.29017561",
         "30.05327759",
         "0.01794713",
         "0.031829834",
         "16.63584389",
         "26.85015259",
         "0.014944964",
         "0.056152344",
         "6.065816423",
         "0",
         "0",
         "0"
        ],
        [
         "229",
         "2020-01-10 13:00:00",
         "1594.8164",
         "30.31844482",
         "0.017743543",
         "0.007068634",
         "28.95522022",
         "30.38875732",
         "0.01815553",
         "0.03517151",
         "16.20449767",
         "27.24813232",
         "0.015519574",
         "0.07284546",
         "5.440281734",
         "0",
         "0",
         "0"
        ],
        [
         "230",
         "2020-01-10 14:00:00",
         "1605.2256",
         "30.17086182",
         "0.017547045",
         "0.003110886",
         "28.91316687",
         "30.40523682",
         "0.01824895",
         "0.032180786",
         "15.53195486",
         "27.39742432",
         "0.016234789",
         "0.08578491",
         "4.620177843",
         "0",
         "0",
         "0"
        ],
        [
         "231",
         "2020-01-10 15:00:00",
         "1574.1553",
         "29.72994385",
         "0.017327841",
         "0.000579119",
         "28.83247033",
         "30.09713135",
         "0.018250998",
         "0.030830383",
         "14.63157342",
         "27.19088135",
         "0.016763266",
         "0.10043335",
         "4.054625731",
         "0",
         "0",
         "0"
        ],
        [
         "232",
         "2020-01-10 16:00:00",
         "1486.8083",
         "28.8979126",
         "0.0172203",
         "0.000159323",
         "28.75283346",
         "29.3822876",
         "0.018334191",
         "0.023712158",
         "13.09078929",
         "26.6479126",
         "0.01695327",
         "0.10372925",
         "4.030804539",
         "0",
         "0",
         "0"
        ],
        [
         "233",
         "2020-01-10 17:00:00",
         "1348.405",
         "27.71962891",
         "0.017333623",
         "0.000124276",
         "27.29363342",
         "27.96962891",
         "0.018409368",
         "0.028877258",
         "10.53551752",
         "25.65712891",
         "0.017150518",
         "0.09863281",
         "4.135078962",
         "0",
         "0",
         "0"
        ],
        [
         "234",
         "2020-01-10 18:00:00",
         "1378.5822",
         "26.59087524",
         "0.017675936",
         "0.000120789",
         "23.49085591",
         "26.02837524",
         "0.017912447",
         "0.032577515",
         "9.10795887",
         "24.52837524",
         "0.016958773",
         "0.11416626",
         "4.855592013",
         "0",
         "0",
         "0"
        ],
        [
         "235",
         "2020-01-10 19:00:00",
         "1422.1301",
         "26.14153442",
         "0.017926743",
         "0.000103205",
         "20.84467635",
         "25.08684692",
         "0.017530015",
         "0.027259827",
         "9.244109816",
         "23.89153442",
         "0.016622117",
         "0.11352539",
         "6.157331405",
         "0",
         "0",
         "0"
        ],
        [
         "236",
         "2020-01-10 20:00:00",
         "1391.8517",
         "25.92440186",
         "0.018047808",
         "0.00015986",
         "19.40749699",
         "24.43221436",
         "0.017284868",
         "0.013774872",
         "9.38960553",
         "23.47908936",
         "0.016445635",
         "0.09643555",
         "6.341736582",
         "0",
         "0",
         "0"
        ],
        [
         "237",
         "2020-01-10 21:00:00",
         "1328.3746",
         "25.77193604",
         "0.01816142",
         "0.000746965",
         "18.16424539",
         "23.95162354",
         "0.01715434",
         "0.006074905",
         "9.218348864",
         "23.22506104",
         "0.016299848",
         "0.08078003",
         "6.327311959",
         "0",
         "0",
         "0"
        ],
        [
         "238",
         "2020-01-10 22:00:00",
         "1295.3931",
         "25.66884766",
         "0.018237239",
         "0.003096581",
         "17.61278092",
         "23.59072266",
         "0.017024165",
         "0.002443314",
         "8.893156854",
         "23.03603516",
         "0.016169673",
         "0.07107544",
         "6.259151948",
         "0",
         "0",
         "0"
        ],
        [
         "239",
         "2020-01-10 23:00:00",
         "1236.2925",
         "25.62478027",
         "0.018275106",
         "0.005878449",
         "17.01411191",
         "23.30446777",
         "0.016894186",
         "0.00094986",
         "8.582279348",
         "22.88259277",
         "0.016108358",
         "0.06359863",
         "6.042535632",
         "0",
         "0",
         "0"
        ],
        [
         "240",
         "2020-01-11 00:00:00",
         "1175.9983",
         "25.61116943",
         "0.01832064",
         "0.006551743",
         "16.31499181",
         "23.04866943",
         "0.01682528",
         "0.00207901",
         "8.205573058",
         "22.75960693",
         "0.016123375",
         "0.06466675",
         "5.799878037",
         "0",
         "0",
         "0"
        ],
        [
         "241",
         "2020-01-11 01:00:00",
         "1142.0389",
         "25.59804688",
         "0.018381491",
         "0.007669449",
         "15.94134716",
         "22.83242188",
         "0.0167488",
         "0.004520416",
         "7.990424768",
         "22.66054688",
         "0.016161337",
         "0.063446045",
         "5.36692283",
         "0",
         "0",
         "0"
        ],
        [
         "242",
         "2020-01-11 02:00:00",
         "1093.421",
         "25.5864502",
         "0.01842713",
         "0.008686066",
         "15.98709044",
         "22.6177002",
         "0.01666474",
         "0.004247665",
         "7.396363262",
         "22.5786377",
         "0.01615357",
         "0.06323242",
         "5.016347239",
         "0",
         "0",
         "0"
        ],
        [
         "243",
         "2020-01-11 03:00:00",
         "1067.8764",
         "25.58236084",
         "0.018427026",
         "0.008197784",
         "16.18057307",
         "22.47298584",
         "0.016588341",
         "0.006120682",
         "7.017052316",
         "22.52767334",
         "0.016145837",
         "0.05870056",
         "5.001738944",
         "0",
         "0",
         "0"
        ],
        [
         "244",
         "2020-01-11 04:00:00",
         "1059.1096",
         "25.56276855",
         "0.018419258",
         "0.007482529",
         "16.4009758",
         "22.42995605",
         "0.016542427",
         "0.007049561",
         "7.170229662",
         "22.49245605",
         "0.016084664",
         "0.05656433",
         "4.865805033",
         "0",
         "0",
         "0"
        ],
        [
         "245",
         "2020-01-11 05:00:00",
         "1054.0637",
         "25.54296265",
         "0.0183886",
         "0.00787735",
         "16.59716472",
         "22.40233765",
         "0.01649651",
         "0.006052017",
         "7.041410505",
         "22.44921265",
         "0.016031116",
         "0.056625366",
         "4.769165601",
         "0",
         "0",
         "0"
        ],
        [
         "246",
         "2020-01-11 06:00:00",
         "1049.2645",
         "25.58117065",
         "0.018388571",
         "0.008041382",
         "16.55224052",
         "22.49523315",
         "0.01651937",
         "0.004428864",
         "7.086014728",
         "22.40148315",
         "0.01600057",
         "0.058135986",
         "4.727452813",
         "0",
         "0",
         "0"
        ],
        [
         "247",
         "2020-01-11 07:00:00",
         "1116.2854",
         "26.45803223",
         "0.018648151",
         "0.008014679",
         "18.79826746",
         "24.15334473",
         "0.017107014",
         "0.012268066",
         "10.02327808",
         "22.91115723",
         "0.016222004",
         "0.05860901",
         "4.826793889",
         "0",
         "0",
         "0"
        ],
        [
         "248",
         "2020-01-11 08:00:00",
         "1244.8815",
         "27.54256592",
         "0.01861034",
         "0.013442993",
         "21.76287621",
         "26.19100342",
         "0.017916065",
         "0.021713257",
         "11.54633749",
         "24.15975342",
         "0.016321521",
         "0.05253601",
         "4.979621895",
         "0",
         "0",
         "0"
        ],
        [
         "249",
         "2020-01-11 09:00:00",
         "1337.3807",
         "28.48449097",
         "0.018702418",
         "0.022850037",
         "22.95037052",
         "27.99230347",
         "0.01830569",
         "0.019439697",
         "13.31907279",
         "25.50792847",
         "0.016337305",
         "0.04838562",
         "4.682508799",
         "0",
         "0",
         "0"
        ],
        [
         "250",
         "2020-01-11 10:00:00",
         "1431.9412",
         "29.37703857",
         "0.01874138",
         "0.03250122",
         "23.13143613",
         "29.26766357",
         "0.018527757",
         "0.015716553",
         "13.50141167",
         "26.69735107",
         "0.01636101",
         "0.047042847",
         "4.358989044",
         "0",
         "0",
         "0"
        ],
        [
         "251",
         "2020-01-11 11:00:00",
         "1444.1931",
         "30.13048706",
         "0.018611113",
         "0.03604126",
         "22.75765597",
         "30.23986206",
         "0.018687407",
         "0.017166138",
         "13.1504983",
         "27.68517456",
         "0.016535917",
         "0.05041504",
         "3.531876318",
         "0",
         "0",
         "0"
        ],
        [
         "252",
         "2020-01-11 12:00:00",
         "1411.2386",
         "30.59060059",
         "0.018443821",
         "0.027900696",
         "22.1823379",
         "30.84060059",
         "0.018802403",
         "0.025230408",
         "12.50441392",
         "28.25466309",
         "0.017085789",
         "0.054519653",
         "2.469595818",
         "0",
         "0",
         "0"
        ],
        [
         "253",
         "2020-01-11 13:00:00",
         "1386.7401",
         "30.81340942",
         "0.018284734",
         "0.016220093",
         "21.38323238",
         "31.12590942",
         "0.018872198",
         "0.03035736",
         "11.75495522",
         "28.62590942",
         "0.017002996",
         "0.06384277",
         "2.054113061",
         "0",
         "0",
         "0"
        ],
        [
         "254",
         "2020-01-11 14:00:00",
         "1384.5795",
         "30.82424316",
         "0.018147193",
         "0.009250641",
         "20.61775201",
         "31.10549316",
         "0.018894874",
         "0.029563904",
         "11.04755272",
         "28.61330566",
         "0.017086707",
         "0.07141113",
         "1.808633389",
         "0",
         "0",
         "0"
        ],
        [
         "255",
         "2020-01-11 15:00:00",
         "1362.5339",
         "30.55776367",
         "0.018039737",
         "0.006631851",
         "20.22961193",
         "30.80776367",
         "0.018947635",
         "0.027252197",
         "10.31780297",
         "28.20620117",
         "0.017314944",
         "0.07064819",
         "1.808634244",
         "0",
         "0",
         "0"
        ],
        [
         "256",
         "2020-01-11 16:00:00",
         "1338.548",
         "29.92781982",
         "0.018069107",
         "0.005958557",
         "19.99209051",
         "30.16219482",
         "0.019190628",
         "0.0317688",
         "9.361024459",
         "27.59969482",
         "0.017496902",
         "0.061950684",
         "1.786566021",
         "0",
         "0",
         "0"
        ],
        [
         "257",
         "2020-01-11 17:00:00",
         "1292.2322",
         "28.80062256",
         "0.018396618",
         "0.014564514",
         "19.2719104",
         "28.76937256",
         "0.01975465",
         "0.035247803",
         "7.222384608",
         "26.60531006",
         "0.017755749",
         "0.064971924",
         "1.746572266",
         "0",
         "0",
         "0"
        ],
        [
         "258",
         "2020-01-11 18:00:00",
         "1327.6568",
         "27.52730713",
         "0.018831484",
         "0.015003204",
         "17.96054881",
         "26.76168213",
         "0.019418947",
         "0.03894043",
         "5.773932182",
         "25.30074463",
         "0.018228762",
         "0.088134766",
         "1.804323571",
         "0",
         "0",
         "0"
        ],
        [
         "259",
         "2020-01-11 19:00:00",
         "1360.332",
         "26.82747803",
         "0.018984243",
         "0.004489899",
         "17.41999957",
         "26.04622803",
         "0.018938467",
         "0.03944397",
         "6.111074785",
         "24.53841553",
         "0.017679617",
         "0.091156006",
         "2.648699879",
         "0",
         "0",
         "0"
        ],
        [
         "260",
         "2020-01-11 20:00:00",
         "1315.7314",
         "26.41283569",
         "0.018931193",
         "0.004896164",
         "16.37212936",
         "25.45971069",
         "0.018626017",
         "0.03907776",
         "5.920768898",
         "23.95971069",
         "0.017420573",
         "0.09249878",
         "3.467736276",
         "0",
         "0",
         "0"
        ],
        [
         "261",
         "2020-01-11 21:00:00",
         "1260.4177",
         "26.1904541",
         "0.018786687",
         "0.005828857",
         "16.01876081",
         "24.9951416",
         "0.0183747",
         "0.037704468",
         "5.852912765",
         "23.5263916",
         "0.017329473",
         "0.08987427",
         "3.780293791",
         "0",
         "0",
         "0"
        ],
        [
         "262",
         "2020-01-11 22:00:00",
         "1219.2241",
         "26.01552734",
         "0.018664926",
         "0.005802155",
         "16.23439875",
         "24.64052734",
         "0.018161386",
         "0.032043457",
         "6.282533766",
         "23.21865234",
         "0.017276376",
         "0.08822632",
         "3.856095977",
         "0",
         "0",
         "0"
        ],
        [
         "263",
         "2020-01-11 23:00:00",
         "1179.1118",
         "25.86184082",
         "0.018573431",
         "0.00875473",
         "16.64071842",
         "24.37746582",
         "0.017947821",
         "0.026405334",
         "6.741996063",
         "23.04152832",
         "0.017230658",
         "0.088012695",
         "4.075764719",
         "0",
         "0",
         "0"
        ],
        [
         "264",
         "2020-01-12 00:00:00",
         "1148.3761",
         "25.75927124",
         "0.018497173",
         "0.017036438",
         "17.26261446",
         "24.13427124",
         "0.017802898",
         "0.022224426",
         "6.952101325",
         "22.84520874",
         "0.017162029",
         "0.087677",
         "4.335352057",
         "0",
         "0",
         "0"
        ],
        [
         "265",
         "2020-01-12 01:00:00",
         "1116.6489",
         "25.70549927",
         "0.018458068",
         "0.017974854",
         "18.40875584",
         "23.93206177",
         "0.01767224",
         "0.017494202",
         "7.190786188",
         "22.65081177",
         "0.017069519",
         "0.088409424",
         "4.600202869",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 168
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target</th>\n",
       "      <th>T2M_toc</th>\n",
       "      <th>QV2M_toc</th>\n",
       "      <th>TQL_toc</th>\n",
       "      <th>W2M_toc</th>\n",
       "      <th>T2M_san</th>\n",
       "      <th>QV2M_san</th>\n",
       "      <th>TQL_san</th>\n",
       "      <th>W2M_san</th>\n",
       "      <th>T2M_dav</th>\n",
       "      <th>QV2M_dav</th>\n",
       "      <th>TQL_dav</th>\n",
       "      <th>W2M_dav</th>\n",
       "      <th>holiday</th>\n",
       "      <th>school</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2020-01-10 00:00:00</td>\n",
       "      <td>1131.5281</td>\n",
       "      <td>24.929865</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>21.301868</td>\n",
       "      <td>23.750177</td>\n",
       "      <td>0.016995</td>\n",
       "      <td>0.003576</td>\n",
       "      <td>10.696918</td>\n",
       "      <td>22.984552</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0.075134</td>\n",
       "      <td>6.629415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2020-01-10 01:00:00</td>\n",
       "      <td>1093.7796</td>\n",
       "      <td>24.922937</td>\n",
       "      <td>0.017270</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>21.954500</td>\n",
       "      <td>23.501062</td>\n",
       "      <td>0.016682</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>11.579647</td>\n",
       "      <td>22.813562</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>0.072235</td>\n",
       "      <td>6.986607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2020-01-10 02:00:00</td>\n",
       "      <td>1061.8127</td>\n",
       "      <td>24.971002</td>\n",
       "      <td>0.017233</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>22.545584</td>\n",
       "      <td>23.197565</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>12.126106</td>\n",
       "      <td>22.596002</td>\n",
       "      <td>0.015642</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>7.229447</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2020-01-10 03:00:00</td>\n",
       "      <td>1037.6880</td>\n",
       "      <td>25.026514</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>23.026767</td>\n",
       "      <td>22.940576</td>\n",
       "      <td>0.015960</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>12.278517</td>\n",
       "      <td>22.370264</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.055237</td>\n",
       "      <td>7.265826</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2020-01-10 04:00:00</td>\n",
       "      <td>1038.8560</td>\n",
       "      <td>25.145166</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>23.509849</td>\n",
       "      <td>22.692041</td>\n",
       "      <td>0.015527</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>12.065393</td>\n",
       "      <td>22.152979</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.048904</td>\n",
       "      <td>7.164663</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2020-01-16 19:00:00</td>\n",
       "      <td>1424.0133</td>\n",
       "      <td>26.575769</td>\n",
       "      <td>0.019315</td>\n",
       "      <td>0.007227</td>\n",
       "      <td>21.104333</td>\n",
       "      <td>25.677332</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.015354</td>\n",
       "      <td>9.955149</td>\n",
       "      <td>24.091394</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>0.089203</td>\n",
       "      <td>5.120333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2020-01-16 20:00:00</td>\n",
       "      <td>1394.5532</td>\n",
       "      <td>26.307031</td>\n",
       "      <td>0.019453</td>\n",
       "      <td>0.011784</td>\n",
       "      <td>21.419926</td>\n",
       "      <td>25.267969</td>\n",
       "      <td>0.017950</td>\n",
       "      <td>0.017555</td>\n",
       "      <td>10.111186</td>\n",
       "      <td>23.572656</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.094604</td>\n",
       "      <td>5.211394</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2020-01-16 21:00:00</td>\n",
       "      <td>1361.0084</td>\n",
       "      <td>26.213312</td>\n",
       "      <td>0.019370</td>\n",
       "      <td>0.015614</td>\n",
       "      <td>22.201817</td>\n",
       "      <td>25.025812</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>9.926674</td>\n",
       "      <td>23.369562</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>0.092407</td>\n",
       "      <td>5.004218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>2020-01-16 22:00:00</td>\n",
       "      <td>1311.0800</td>\n",
       "      <td>26.126337</td>\n",
       "      <td>0.019334</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>22.534374</td>\n",
       "      <td>24.923212</td>\n",
       "      <td>0.018021</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>10.030370</td>\n",
       "      <td>23.282587</td>\n",
       "      <td>0.017220</td>\n",
       "      <td>0.094116</td>\n",
       "      <td>5.005732</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>2020-01-16 23:00:00</td>\n",
       "      <td>1249.8618</td>\n",
       "      <td>26.040094</td>\n",
       "      <td>0.019373</td>\n",
       "      <td>0.015953</td>\n",
       "      <td>22.522581</td>\n",
       "      <td>24.844781</td>\n",
       "      <td>0.018114</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>9.963091</td>\n",
       "      <td>23.235406</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.091675</td>\n",
       "      <td>5.141000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp     target    T2M_toc  QV2M_toc   TQL_toc    W2M_toc  \\\n",
       "216 2020-01-10 00:00:00  1131.5281  24.929865  0.017285  0.000013  21.301868   \n",
       "217 2020-01-10 01:00:00  1093.7796  24.922937  0.017270  0.001275  21.954500   \n",
       "218 2020-01-10 02:00:00  1061.8127  24.971002  0.017233  0.001014  22.545584   \n",
       "219 2020-01-10 03:00:00  1037.6880  25.026514  0.017226  0.001494  23.026767   \n",
       "220 2020-01-10 04:00:00  1038.8560  25.145166  0.017373  0.005375  23.509849   \n",
       "..                  ...        ...        ...       ...       ...        ...   \n",
       "379 2020-01-16 19:00:00  1424.0133  26.575769  0.019315  0.007227  21.104333   \n",
       "380 2020-01-16 20:00:00  1394.5532  26.307031  0.019453  0.011784  21.419926   \n",
       "381 2020-01-16 21:00:00  1361.0084  26.213312  0.019370  0.015614  22.201817   \n",
       "382 2020-01-16 22:00:00  1311.0800  26.126337  0.019334  0.015160  22.534374   \n",
       "383 2020-01-16 23:00:00  1249.8618  26.040094  0.019373  0.015953  22.522581   \n",
       "\n",
       "       T2M_san  QV2M_san   TQL_san    W2M_san    T2M_dav  QV2M_dav   TQL_dav  \\\n",
       "216  23.750177  0.016995  0.003576  10.696918  22.984552  0.016171  0.075134   \n",
       "217  23.501062  0.016682  0.004702  11.579647  22.813562  0.015920  0.072235   \n",
       "218  23.197565  0.016317  0.002250  12.126106  22.596002  0.015642  0.065491   \n",
       "219  22.940576  0.015960  0.002110  12.278517  22.370264  0.015418  0.055237   \n",
       "220  22.692041  0.015527  0.002665  12.065393  22.152979  0.015206  0.048904   \n",
       "..         ...       ...       ...        ...        ...       ...       ...   \n",
       "379  25.677332  0.018018  0.015354   9.955149  24.091394  0.017201  0.089203   \n",
       "380  25.267969  0.017950  0.017555  10.111186  23.572656  0.017172  0.094604   \n",
       "381  25.025812  0.017951  0.017937   9.926674  23.369562  0.017188  0.092407   \n",
       "382  24.923212  0.018021  0.011055  10.030370  23.282587  0.017220  0.094116   \n",
       "383  24.844781  0.018114  0.005102   9.963091  23.235406  0.017275  0.091675   \n",
       "\n",
       "      W2M_dav  holiday  school  item_id  \n",
       "216  6.629415        0       0        0  \n",
       "217  6.986607        0       0        0  \n",
       "218  7.229447        0       0        0  \n",
       "219  7.265826        0       0        0  \n",
       "220  7.164663        0       0        0  \n",
       "..        ...      ...     ...      ...  \n",
       "379  5.120333        0       0        0  \n",
       "380  5.211394        0       0        0  \n",
       "381  5.004218        0       0        0  \n",
       "382  5.005732        0       0        0  \n",
       "383  5.141000        0       0        0  \n",
       "\n",
       "[168 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_mask = (test_df['timestamp'] >= '2020-01-10') & (test_df['timestamp'] < '2020-01-17')\n",
    "\n",
    "test_df[date_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sk_m\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "def calculate_sklearn_metrics(df: pd.DataFrame,\n",
    "                            target_column: str = 'target',\n",
    "                           forecast_cols: List[str] = ['0.1', '0.5', '0.9'],\n",
    "                           naive_forecast_col: Optional[str] = None,\n",
    "                           metrics: List[str] = ['MASE', 'MAPE', 'MSE', 'MAE', 'SQL']) -> Dict[str, float]:\n",
    "    for col in forecast_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Столбец с прогнозом '{col}' не найден в датафрейме\")\n",
    "    \n",
    "    if 'MASE' in metrics and naive_forecast_col is None:\n",
    "        df['naive_forecast'] = df['0.5'].shift(1)\n",
    "        naive_forecast_col = 'naive_forecast'\n",
    "    \n",
    "    df = df.dropna(subset=['0.5'] + ([naive_forecast_col] if naive_forecast_col else []))\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"После удаления NaN значений датафрейм пуст\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    y_true = df[target_column].values\n",
    "    y_pred = df['0.5'].values\n",
    "    \n",
    "    if 'MSE' in metrics:\n",
    "        results['MSE'] = sk_m.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    if 'MAE' in metrics:\n",
    "        results['MAE'] = sk_m.mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    if 'MAPE' in metrics:\n",
    "        mask = y_true != 0\n",
    "        if not np.any(mask):\n",
    "            results['MAPE'] = np.nan\n",
    "        else:\n",
    "            mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "            results['MAPE'] = mape\n",
    "    \n",
    "    if 'MASE' in metrics:\n",
    "        if naive_forecast_col not in df.columns:\n",
    "            results['MASE'] = np.nan\n",
    "        else:\n",
    "            naive_errors = np.abs(df['0.5'].values[1:] - df[naive_forecast_col].values[1:])\n",
    "            denominator = np.mean(naive_errors)\n",
    "            \n",
    "            if denominator == 0:\n",
    "                results['MASE'] = np.nan\n",
    "            else:\n",
    "                numerator = sk_m.mean_absolute_error(y_true, y_pred)\n",
    "                results['MASE'] = numerator / denominator\n",
    "    \n",
    "    if 'SQL' in metrics:\n",
    "        sql_losses = []\n",
    "        for forecast_col in forecast_cols:\n",
    "            y_pred = df[forecast_col].values\n",
    "            \n",
    "            try:\n",
    "                quantile = float(forecast_col)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            errors = y_true - y_pred\n",
    "            sql_loss = np.mean(np.maximum(quantile * errors, (quantile - 1) * errors))\n",
    "            sql_losses.append(sql_loss)\n",
    "        \n",
    "        if sql_losses:\n",
    "            results['SQL'] = sum(sql_losses) / len(sql_losses)\n",
    "        else:\n",
    "            results['SQL'] = np.nan\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run ChronosFineTuned[bolt_small]_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/4af835f2cf374245bb02cf5f41defa06\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n",
      "🏃 View run ChronosZeroShot[bolt_small]_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/f72def3e767749159d564f28a3c92c22\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n",
      "🏃 View run RecursiveTabular_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/0e58760991fc4b0b8d3b74b62e2448d0\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n",
      "🏃 View run DirectTabular_AllData at: http://127.0.0.1:5000/#/experiments/185045746886025740/runs/e48faa94445f4b7588d7ac9ed629c72b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/185045746886025740\n"
     ]
    }
   ],
   "source": [
    "prefix = 'AllData'\n",
    "\n",
    "for k, v in all_models_predictions.items():\n",
    "    run_name = f\"{k}_{prefix}\"\n",
    "\n",
    "    pred_df = pd.DataFrame(v).reset_index(drop=True)\n",
    "    pred_df['target'] = test_df[:config.prediction_length * max_iterations]['target'].values\n",
    "    _metrics = calculate_sklearn_metrics(pred_df)\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_metrics(_metrics)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "        mlflow.set_tag(\"prefix\", prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 5496.380038097112,\n",
       " 'MAE': 53.8071345690901,\n",
       " 'MAPE': 4.216594596034557,\n",
       " 'MASE': 1.2343038856109387,\n",
       " 'SQL': 17.909953705210327}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(all_models_predictions['ChronosFineTuned[bolt_small]'].reset_index(drop=True))\n",
    "pred_df['target'] = test_df[:config.prediction_length * max_iterations]['target'].values\n",
    "\n",
    "calculate_sklearn_metrics(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_models_predictions['ChronosFineTuned[bolt_small]'].reset_index(drop=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [720, 719]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m y_true \u001b[38;5;241m=\u001b[39m test_df[:config\u001b[38;5;241m.\u001b[39mprediction_length \u001b[38;5;241m*\u001b[39m max_iterations][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m----> 2\u001b[0m calculate_sklearn_metrics(pd\u001b[38;5;241m.\u001b[39mDataFrame(all_models_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChronosFineTuned[bolt_small]\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)), y_true\u001b[38;5;241m=\u001b[39my_true)\n",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m, in \u001b[0;36mcalculate_sklearn_metrics\u001b[0;34m(df, y_true, forecast_cols, naive_forecast_col, metrics)\u001b[0m\n\u001b[1;32m     26\u001b[0m y_pred_05 \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.5\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m---> 29\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sk_m\u001b[38;5;241m.\u001b[39mmean_squared_error(y_true, y_pred_05)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m     32\u001b[0m     results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sk_m\u001b[38;5;241m.\u001b[39mmean_absolute_error(y_true, y_pred_05)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:506\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    503\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    504\u001b[0m         )\n\u001b[0;32m--> 506\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    507\u001b[0m     y_true, y_pred, multioutput\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    509\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    510\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_regression.py:111\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m--> 111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [720, 719]"
     ]
    }
   ],
   "source": [
    "y_true = test_df[:config.prediction_length * max_iterations]['target'].values\n",
    "calculate_sklearn_metrics(pd.DataFrame(all_models_predictions['ChronosFineTuned[bolt_small]'].reset_index(drop=True)), y_true=y_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
