{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import mlflow\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import plotly.graph_objects as go\n",
    "from huggingface_hub import login\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from utils import calculate_sklearn_metrics, TrainingConfig\n",
    "\n",
    "dotenv.load_dotenv(\"../../.env\")\n",
    "\n",
    "token = os.environ[\"HF_TOKEN\"]\n",
    "login(token=token)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"rosstat_forecasting\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 4140 строк\n",
      "Валидационная выборка: 828 строк\n",
      "Тестовая выборка: 828 строк\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../../data/rosstat/processed'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train/data.csv'))\n",
    "val_df = pd.read_csv(os.path.join(data_dir, 'val/data.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test/data.csv'))\n",
    "\n",
    "print(f\"Обучающая выборка: {train_df.shape[0]} строк\")\n",
    "print(f\"Валидационная выборка: {val_df.shape[0]} строк\")\n",
    "print(f\"Тестовая выборка: {test_df.shape[0]} строк\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    train_df.rename(columns={\"nominal_wage\": \"target\"}),\n",
    "    id_column=\"code\",\n",
    "    timestamp_column=\"date\",\n",
    ")\n",
    "val_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    val_df.rename(columns={\"nominal_wage\": \"target\"}),\n",
    "    id_column=\"code\",\n",
    "    timestamp_column=\"date\",\n",
    ")\n",
    "test_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    test_df.rename(columns={\"nominal_wage\": \"target\"}),\n",
    "    id_column=\"code\",\n",
    "    timestamp_column=\"date\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/auto_ml_single_target_bigger\"\n",
      "Beginning AutoGluon training...\n",
      "AutoGluon will save models to '/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.0\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #61~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 15 17:03:15 UTC 2\n",
      "CPU Count:          12\n",
      "GPU Count:          1\n",
      "Memory Avail:       17.86 GB / 30.95 GB (57.7%)\n",
      "Disk Space Avail:   113.93 GB / 233.67 GB (48.8%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': False,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': 'MS',\n",
      " 'hyperparameters': {'Chronos': [{'ag_args': {'name_suffix': 'ZeroShot'},\n",
      "                                  'model_path': 'bolt_tiny'},\n",
      "                                 {'ag_args': {'name_suffix': 'FineTuned'},\n",
      "                                  'fine_tune': True,\n",
      "                                  'model_path': 'bolt_tiny'},\n",
      "                                 {'ag_args': {'name_suffix': 'ZeroShot'},\n",
      "                                  'model_path': 'bolt_mini'},\n",
      "                                 {'ag_args': {'name_suffix': 'FineTuned'},\n",
      "                                  'fine_tune': True,\n",
      "                                  'model_path': 'bolt_mini'},\n",
      "                                 {'ag_args': {'name_suffix': 'ZeroShot'},\n",
      "                                  'model_path': 'bolt_small'},\n",
      "                                 {'ag_args': {'name_suffix': 'FineTuned'},\n",
      "                                  'fine_tune': True,\n",
      "                                  'model_path': 'bolt_small'},\n",
      "                                 {'ag_args': {'name_suffix': 'ZeroShot'},\n",
      "                                  'model_path': 'bolt_base'},\n",
      "                                 {'ag_args': {'name_suffix': 'FineTuned'},\n",
      "                                  'fine_tune': True,\n",
      "                                  'model_path': 'bolt_base'}]},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 2,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'verbosity': 4}\n",
      "\n",
      "Provided train_data has 4140 rows, 69 time series. Median time series length is 60 (min=60, max=60). \n",
      "Provided tuning_data has 828 rows, 69 time series. Median time series length is 12 (min=12, max=12). \n",
      "\tSetting num_val_windows = 0 (disabling backtesting on train_data) because tuning_data is provided.\n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\tpast_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['capital_labor_ratio_change', 'capital_productivity_change', 'fixed_assets...arable_prices', 'labor_productivity', 'high_productivity_jobs', 'machinery_sh..._total_assets', ...]\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "Removing existing cached predictions file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-05-11 22:16:55\n",
      "Models that will be trained: ['ChronosZeroShot[bolt_tiny]', 'ChronosFineTuned[bolt_tiny]', 'ChronosZeroShot[bolt_mini]', 'ChronosFineTuned[bolt_mini]', 'ChronosZeroShot[bolt_small]', 'ChronosFineTuned[bolt_small]', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_base]']\n",
      "Training timeseries model ChronosZeroShot[bolt_tiny]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-tiny.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "\t-0.1372       = Validation score (-WQL)\n",
      "\t2.18    s     = Training runtime\n",
      "\t1.93    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_tiny]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-tiny.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Transformers logging is turned on during fine-tuning. Note that losses reported by transformers may not correspond to those specified via `eval_metric`.\n",
      "***** Running training *****\n",
      "  Num examples = 32,000\n",
      "  Num Epochs = 9,223,372,036,854,775,807\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 8,652,672\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 10999701.76, 'grad_norm': 26.469579696655273, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 10521255.04, 'grad_norm': 182.75479125976562, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 11352753.92, 'grad_norm': 8.0101900100708, 'learning_rate': 7e-06, 'epoch': 0.3}\n",
      "{'loss': 8839262.08, 'grad_norm': 197.45179748535156, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 9787301.12, 'grad_norm': 395.4043884277344, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 9471229.44, 'grad_norm': 497.9562072753906, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 10540835.2, 'grad_norm': 218.26736450195312, 'learning_rate': 3e-06, 'epoch': 0.7}\n",
      "{'loss': 12419335.68, 'grad_norm': 331.5768127441406, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n",
      "{'loss': 7465043.2, 'grad_norm': 320.19500732421875, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.9}\n",
      "{'loss': 7760501.76, 'grad_norm': 567.8292846679688, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 32\n",
      "{'eval_loss': 4.3714189529418945, 'eval_runtime': 0.0238, 'eval_samples_per_second': 2895.778, 'eval_steps_per_second': 125.903, 'epoch': 1.0}\n",
      "Saving model checkpoint to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/transformers_logs/checkpoint-1000\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/transformers_logs/checkpoint-1000/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/transformers_logs/checkpoint-1000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 18.363, 'train_samples_per_second': 1742.636, 'train_steps_per_second': 54.457, 'train_loss': 9915721.92, 'epoch': 1.0}\n",
      "\tSaving fine-tuned model to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt/model.safetensors\n",
      "Removing transformers_logs directory /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/transformers_logs\n",
      "\t-0.1220       = Validation score (-WQL)\n",
      "\t19.45   s     = Training runtime\n",
      "\t0.04    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_mini]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\t-0.1349       = Validation score (-WQL)\n",
      "\t0.00    s     = Training runtime\n",
      "\t1.29    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_mini]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Transformers logging is turned on during fine-tuning. Note that losses reported by transformers may not correspond to those specified via `eval_metric`.\n",
      "***** Running training *****\n",
      "  Num examples = 32,000\n",
      "  Num Epochs = 9,223,372,036,854,775,807\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 21,236,096\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 10999701.76, 'grad_norm': 284.6534118652344, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 10521249.28, 'grad_norm': 234.69163513183594, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 11352750.08, 'grad_norm': 163.37832641601562, 'learning_rate': 7e-06, 'epoch': 0.3}\n",
      "{'loss': 8839254.4, 'grad_norm': 251.11097717285156, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 9787290.24, 'grad_norm': 544.8069458007812, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 9471218.56, 'grad_norm': 388.94378662109375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 10540825.6, 'grad_norm': 262.2857666015625, 'learning_rate': 3e-06, 'epoch': 0.7}\n",
      "{'loss': 12419319.04, 'grad_norm': 516.04248046875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n",
      "{'loss': 7465029.12, 'grad_norm': 343.3749084472656, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.9}\n",
      "{'loss': 7760485.76, 'grad_norm': 719.6192626953125, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 32\n",
      "{'eval_loss': 4.182464122772217, 'eval_runtime': 0.021, 'eval_samples_per_second': 3283.827, 'eval_steps_per_second': 142.775, 'epoch': 1.0}\n",
      "Saving model checkpoint to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/transformers_logs/checkpoint-1000\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/transformers_logs/checkpoint-1000/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/transformers_logs/checkpoint-1000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 18.7562, 'train_samples_per_second': 1706.099, 'train_steps_per_second': 53.316, 'train_loss': 9915712.384, 'epoch': 1.0}\n",
      "\tSaving fine-tuned model to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt/model.safetensors\n",
      "Removing transformers_logs directory /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/transformers_logs\n",
      "\t-0.1208       = Validation score (-WQL)\n",
      "\t19.95   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_small]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\t-0.1342       = Validation score (-WQL)\n",
      "\t0.00    s     = Training runtime\n",
      "\t1.22    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_small]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Transformers logging is turned on during fine-tuning. Note that losses reported by transformers may not correspond to those specified via `eval_metric`.\n",
      "***** Running training *****\n",
      "  Num examples = 32,000\n",
      "  Num Epochs = 9,223,372,036,854,775,807\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 47,718,016\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 10999699.2, 'grad_norm': 313.139892578125, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 10521247.36, 'grad_norm': 318.7878723144531, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 11352750.08, 'grad_norm': 4.392320156097412, 'learning_rate': 7e-06, 'epoch': 0.3}\n",
      "{'loss': 8839253.76, 'grad_norm': 295.3804626464844, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 9787286.4, 'grad_norm': 813.7857055664062, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 9471213.44, 'grad_norm': 419.2200622558594, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 10540820.48, 'grad_norm': 304.8425598144531, 'learning_rate': 3e-06, 'epoch': 0.7}\n",
      "{'loss': 12419307.52, 'grad_norm': 543.9916381835938, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n",
      "{'loss': 7465020.8, 'grad_norm': 498.33685302734375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.9}\n",
      "{'loss': 7760472.96, 'grad_norm': 868.4036865234375, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 32\n",
      "{'eval_loss': 4.224575996398926, 'eval_runtime': 0.026, 'eval_samples_per_second': 2650.296, 'eval_steps_per_second': 115.23, 'epoch': 1.0}\n",
      "Saving model checkpoint to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/transformers_logs/checkpoint-1000\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/transformers_logs/checkpoint-1000/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/transformers_logs/checkpoint-1000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 30.9526, 'train_samples_per_second': 1033.84, 'train_steps_per_second': 32.307, 'train_loss': 9915707.2, 'epoch': 1.0}\n",
      "\tSaving fine-tuned model to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt/model.safetensors\n",
      "Removing transformers_logs directory /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/transformers_logs\n",
      "\t-0.1205       = Validation score (-WQL)\n",
      "\t32.35   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosZeroShot[bolt_base]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\t-0.1365       = Validation score (-WQL)\n",
      "\t0.00    s     = Training runtime\n",
      "\t2.49    s     = Validation (prediction) runtime\n",
      "Training timeseries model ChronosFineTuned[bolt_base]. \n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Transformers logging is turned on during fine-tuning. Note that losses reported by transformers may not correspond to those specified via `eval_metric`.\n",
      "***** Running training *****\n",
      "  Num examples = 32,000\n",
      "  Num Epochs = 9,223,372,036,854,775,807\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 205,292,928\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 10999699.2, 'grad_norm': 401.1920471191406, 'learning_rate': 9e-06, 'epoch': 0.1}\n",
      "{'loss': 10521244.8, 'grad_norm': 367.9422607421875, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 11352746.24, 'grad_norm': 3.486124038696289, 'learning_rate': 7e-06, 'epoch': 0.3}\n",
      "{'loss': 8839250.56, 'grad_norm': 391.84088134765625, 'learning_rate': 6e-06, 'epoch': 0.4}\n",
      "{'loss': 9787283.84, 'grad_norm': 856.39453125, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 9471212.16, 'grad_norm': 644.5598754882812, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.6}\n",
      "{'loss': 10540817.92, 'grad_norm': 407.6231994628906, 'learning_rate': 3e-06, 'epoch': 0.7}\n",
      "{'loss': 12419307.52, 'grad_norm': 801.060791015625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n",
      "{'loss': 7465018.24, 'grad_norm': 601.26904296875, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.9}\n",
      "{'loss': 7760474.24, 'grad_norm': 1165.365234375, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 32\n",
      "{'eval_loss': 4.275412082672119, 'eval_runtime': 0.0459, 'eval_samples_per_second': 1502.055, 'eval_steps_per_second': 65.307, 'epoch': 1.0}\n",
      "Saving model checkpoint to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/transformers_logs/checkpoint-1000\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/transformers_logs/checkpoint-1000/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/transformers_logs/checkpoint-1000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 76.2715, 'train_samples_per_second': 419.554, 'train_steps_per_second': 13.111, 'train_loss': 9915705.472, 'epoch': 1.0}\n",
      "\tSaving fine-tuned model to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt\n",
      "Configuration saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt/config.json\n",
      "Model weights saved in /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt/model.safetensors\n",
      "Removing transformers_logs directory /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/transformers_logs\n",
      "\t-0.1219       = Validation score (-WQL)\n",
      "\t85.77   s     = Training runtime\n",
      "\t0.06    s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['ChronosZeroShot[bolt_tiny]', 'ChronosFineTuned[bolt_tiny]', 'ChronosZeroShot[bolt_mini]', 'ChronosFineTuned[bolt_mini]', 'ChronosZeroShot[bolt_small]', 'ChronosFineTuned[bolt_small]', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_base]']\n",
      "Total runtime: 166.84 s\n",
      "Best model: ChronosFineTuned[bolt_small]\n",
      "Best model score: -0.1205\n"
     ]
    }
   ],
   "source": [
    "config = TrainingConfig(\n",
    "    prediction_length=2,  # полгода\n",
    "    artifact_path=\"../models/auto_ml_single_target_bigger\",\n",
    ")\n",
    "\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=config.prediction_length, path=config.artifact_path, freq=\"MS\"\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    tuning_data=val_data,\n",
    "    verbosity=4,\n",
    "    hyperparameters={\n",
    "        \"Chronos\": [\n",
    "            {\"model_path\": \"bolt_tiny\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_tiny\",\n",
    "                \"fine_tune\": True,\n",
    "                \"ag_args\": {\"name_suffix\": \"FineTuned\"},\n",
    "            },\n",
    "            {\"model_path\": \"bolt_mini\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_mini\",\n",
    "                \"fine_tune\": True,\n",
    "                \"ag_args\": {\"name_suffix\": \"FineTuned\"},\n",
    "            },\n",
    "            {\"model_path\": \"bolt_small\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_small\",\n",
    "                \"fine_tune\": True,\n",
    "                \"ag_args\": {\"name_suffix\": \"FineTuned\"},\n",
    "            },\n",
    "            {\"model_path\": \"bolt_base\", \"ag_args\": {\"name_suffix\": \"ZeroShot\"}},\n",
    "            {\n",
    "                \"model_path\": \"bolt_base\",\n",
    "                \"fine_tune\": True,\n",
    "                \"ag_args\": {\"name_suffix\": \"FineTuned\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    enable_ensemble=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading predictor from path /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger\n"
     ]
    }
   ],
   "source": [
    "config = TrainingConfig(\n",
    "    prediction_length=2,  # полгода\n",
    "    artifact_path=\"../models/auto_ml_single_target_bigger\",\n",
    ")\n",
    "\n",
    "predictor = TimeSeriesPredictor.load(config.artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating leaderboard for all models trained\n",
      "Additional data provided, testing on additional data. Resulting leaderboard will be sorted according to test score (`score_test`).\n",
      "Prediction order: ['ChronosZeroShot[bolt_tiny]', 'ChronosZeroShot[bolt_mini]', 'ChronosZeroShot[bolt_small]', 'ChronosFineTuned[bolt_base]', 'ChronosFineTuned[bolt_small]', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_mini]', 'ChronosFineTuned[bolt_tiny]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-tiny/snapshots/590f7666166f6f503e215bf0dac08a68390e0302/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-tiny.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-mini/snapshots/415f92662870b164b5a782f2b1dd6f81a75e3822/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-small/snapshots/94d26f8f14f17f8c7c6ddf01521a959d4722bc6e/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\tFine-tuned checkpoint exists, setting model_path to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_base]/fine-tuned-ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\tFine-tuned checkpoint exists, setting model_path to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-small\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_small]/fine-tuned-ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-base\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/nikita/.cache/huggingface/hub/models--autogluon--chronos-bolt-base/snapshots/7354124e97486213d33dae999b1914c5aff19089/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at autogluon/chronos-bolt-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\tFine-tuned checkpoint exists, setting model_path to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-mini\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1536,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 384,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_mini]/fine-tuned-ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "\tFine-tuned checkpoint exists, setting model_path to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"/home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"auto\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading configuration file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"autogluon/chronos-bolt-tiny\",\n",
      "  \"architectures\": [\n",
      "    \"ChronosBoltModelForForecasting\"\n",
      "  ],\n",
      "  \"chronos_config\": {\n",
      "    \"context_length\": 2048,\n",
      "    \"input_patch_size\": 16,\n",
      "    \"input_patch_stride\": 16,\n",
      "    \"prediction_length\": 64,\n",
      "    \"quantiles\": [\n",
      "      0.1,\n",
      "      0.2,\n",
      "      0.3,\n",
      "      0.4,\n",
      "      0.5,\n",
      "      0.6,\n",
      "      0.7,\n",
      "      0.8,\n",
      "      0.9\n",
      "    ],\n",
      "    \"use_reg_token\": true\n",
      "  },\n",
      "  \"chronos_pipeline_class\": \"ChronosBoltPipeline\",\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 256,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 0.05,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 4,\n",
      "  \"num_heads\": 4,\n",
      "  \"num_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reg_token_id\": 1,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2\n",
      "}\n",
      "\n",
      "loading weights file /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating ChronosBoltModelForForecasting model under default dtype torch.float32.\n",
      "All model checkpoint weights were used when initializing ChronosBoltModelForForecasting.\n",
      "\n",
      "All the weights of ChronosBoltModelForForecasting were initialized from the model checkpoint at /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/ChronosFineTuned[bolt_tiny]/fine-tuned-ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChronosBoltModelForForecasting for predictions without further training.\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n",
      "/home/nikita/anaconda3/lib/python3.12/site-packages/autogluon/timeseries/metrics/abstract.py:101: FutureWarning: Passing `prediction_length` to `TimeSeriesScorer.__call__` is deprecated and will be removed in v2.0. Please set the `eval_metric.prediction_length` attribute instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "WQL_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "WQL_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_test",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pred_time_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fit_time_marginal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fit_order",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "MASE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAPE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SQL",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0e515e54-6825-4904-bd55-738c9ded6f04",
       "rows": [
        [
         "0",
         "ChronosFineTuned[bolt_mini]",
         "-0.12176765855433189",
         "-0.12079955297330612",
         "0.24812722206115723",
         "0.010143041610717773",
         "19.947504997253418",
         "4",
         "-13259.441394927537",
         "-0.11423498831390033",
         "-435562447.98535526",
         "-13259.441394927537",
         "-11419.164102443136"
        ],
        [
         "1",
         "ChronosFineTuned[bolt_base]",
         "-0.12326484270388198",
         "-0.12187352776633405",
         "0.3900001049041748",
         "0.06257843971252441",
         "85.76629900932312",
         "8",
         "-13919.88828125",
         "-0.12189654498542406",
         "-437365694.2529095",
         "-13919.88828125",
         "-11559.567487859806"
        ],
        [
         "2",
         "ChronosFineTuned[bolt_small]",
         "-0.12509035790173853",
         "-0.12051600503505579",
         "0.2715933322906494",
         "0.010703325271606445",
         "32.34896492958069",
         "6",
         "-13888.484725996375",
         "-0.120992815466855",
         "-446017360.0280178",
         "-13888.484725996375",
         "-11730.761201061794"
        ],
        [
         "3",
         "ChronosFineTuned[bolt_tiny]",
         "-0.1313631213266081",
         "-0.12202243322724086",
         "0.24413776397705078",
         "0.03619837760925293",
         "19.447214603424072",
         "2",
         "-13800.150339673914",
         "-0.11810027862114246",
         "-490570395.98063695",
         "-13800.150339673914",
         "-12319.01029589372"
        ],
        [
         "4",
         "ChronosZeroShot[bolt_mini]",
         "-0.13348554310168756",
         "-0.13494696290383532",
         "0.8530788421630859",
         "1.2916955947875977",
         "0.002488851547241211",
         "3",
         "-14264.333939085143",
         "-0.12117006887532117",
         "-557893155.6035408",
         "-14264.333939085143",
         "-12518.04740338164"
        ],
        [
         "5",
         "ChronosZeroShot[bolt_small]",
         "-0.1358277969483997",
         "-0.1341951891423005",
         "0.8353271484375",
         "1.2152700424194336",
         "0.002881765365600586",
         "5",
         "-14345.202858922103",
         "-0.12181289933274969",
         "-563494861.1769836",
         "-14345.202858922103",
         "-12737.699988992052"
        ],
        [
         "6",
         "ChronosZeroShot[bolt_base]",
         "-0.13596031025721622",
         "-0.1364886864885074",
         "0.9770472049713135",
         "2.4867587089538574",
         "0.0028793811798095703",
         "7",
         "-14271.658163496377",
         "-0.12121381550042612",
         "-556591294.1923153",
         "-14271.658163496377",
         "-12750.126861916264"
        ],
        [
         "7",
         "ChronosZeroShot[bolt_tiny]",
         "-0.13707975228473057",
         "-0.1371943706730293",
         "1.0422277450561523",
         "1.9344782829284668",
         "2.1835596561431885",
         "1",
         "-14454.922486413045",
         "-0.12334158856250908",
         "-565142465.1382493",
         "-14454.922486413045",
         "-12855.106233016302"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>WQL_test</th>\n",
       "      <th>WQL_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>fit_order</th>\n",
       "      <th>MASE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>SQL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChronosFineTuned[bolt_mini]</td>\n",
       "      <td>-0.121768</td>\n",
       "      <td>-0.120800</td>\n",
       "      <td>0.248127</td>\n",
       "      <td>0.010143</td>\n",
       "      <td>19.947505</td>\n",
       "      <td>4</td>\n",
       "      <td>-13259.441395</td>\n",
       "      <td>-0.114235</td>\n",
       "      <td>-4.355624e+08</td>\n",
       "      <td>-13259.441395</td>\n",
       "      <td>-11419.164102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChronosFineTuned[bolt_base]</td>\n",
       "      <td>-0.123265</td>\n",
       "      <td>-0.121874</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.062578</td>\n",
       "      <td>85.766299</td>\n",
       "      <td>8</td>\n",
       "      <td>-13919.888281</td>\n",
       "      <td>-0.121897</td>\n",
       "      <td>-4.373657e+08</td>\n",
       "      <td>-13919.888281</td>\n",
       "      <td>-11559.567488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ChronosFineTuned[bolt_small]</td>\n",
       "      <td>-0.125090</td>\n",
       "      <td>-0.120516</td>\n",
       "      <td>0.271593</td>\n",
       "      <td>0.010703</td>\n",
       "      <td>32.348965</td>\n",
       "      <td>6</td>\n",
       "      <td>-13888.484726</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-4.460174e+08</td>\n",
       "      <td>-13888.484726</td>\n",
       "      <td>-11730.761201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ChronosFineTuned[bolt_tiny]</td>\n",
       "      <td>-0.131363</td>\n",
       "      <td>-0.122022</td>\n",
       "      <td>0.244138</td>\n",
       "      <td>0.036198</td>\n",
       "      <td>19.447215</td>\n",
       "      <td>2</td>\n",
       "      <td>-13800.150340</td>\n",
       "      <td>-0.118100</td>\n",
       "      <td>-4.905704e+08</td>\n",
       "      <td>-13800.150340</td>\n",
       "      <td>-12319.010296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ChronosZeroShot[bolt_mini]</td>\n",
       "      <td>-0.133486</td>\n",
       "      <td>-0.134947</td>\n",
       "      <td>0.853079</td>\n",
       "      <td>1.291696</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>3</td>\n",
       "      <td>-14264.333939</td>\n",
       "      <td>-0.121170</td>\n",
       "      <td>-5.578932e+08</td>\n",
       "      <td>-14264.333939</td>\n",
       "      <td>-12518.047403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ChronosZeroShot[bolt_small]</td>\n",
       "      <td>-0.135828</td>\n",
       "      <td>-0.134195</td>\n",
       "      <td>0.835327</td>\n",
       "      <td>1.215270</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>5</td>\n",
       "      <td>-14345.202859</td>\n",
       "      <td>-0.121813</td>\n",
       "      <td>-5.634949e+08</td>\n",
       "      <td>-14345.202859</td>\n",
       "      <td>-12737.699989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ChronosZeroShot[bolt_base]</td>\n",
       "      <td>-0.135960</td>\n",
       "      <td>-0.136489</td>\n",
       "      <td>0.977047</td>\n",
       "      <td>2.486759</td>\n",
       "      <td>0.002879</td>\n",
       "      <td>7</td>\n",
       "      <td>-14271.658163</td>\n",
       "      <td>-0.121214</td>\n",
       "      <td>-5.565913e+08</td>\n",
       "      <td>-14271.658163</td>\n",
       "      <td>-12750.126862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ChronosZeroShot[bolt_tiny]</td>\n",
       "      <td>-0.137080</td>\n",
       "      <td>-0.137194</td>\n",
       "      <td>1.042228</td>\n",
       "      <td>1.934478</td>\n",
       "      <td>2.183560</td>\n",
       "      <td>1</td>\n",
       "      <td>-14454.922486</td>\n",
       "      <td>-0.123342</td>\n",
       "      <td>-5.651425e+08</td>\n",
       "      <td>-14454.922486</td>\n",
       "      <td>-12855.106233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  WQL_test   WQL_val  pred_time_test  \\\n",
       "0   ChronosFineTuned[bolt_mini] -0.121768 -0.120800        0.248127   \n",
       "1   ChronosFineTuned[bolt_base] -0.123265 -0.121874        0.390000   \n",
       "2  ChronosFineTuned[bolt_small] -0.125090 -0.120516        0.271593   \n",
       "3   ChronosFineTuned[bolt_tiny] -0.131363 -0.122022        0.244138   \n",
       "4    ChronosZeroShot[bolt_mini] -0.133486 -0.134947        0.853079   \n",
       "5   ChronosZeroShot[bolt_small] -0.135828 -0.134195        0.835327   \n",
       "6    ChronosZeroShot[bolt_base] -0.135960 -0.136489        0.977047   \n",
       "7    ChronosZeroShot[bolt_tiny] -0.137080 -0.137194        1.042228   \n",
       "\n",
       "   pred_time_val  fit_time_marginal  fit_order          MASE      MAPE  \\\n",
       "0       0.010143          19.947505          4 -13259.441395 -0.114235   \n",
       "1       0.062578          85.766299          8 -13919.888281 -0.121897   \n",
       "2       0.010703          32.348965          6 -13888.484726 -0.120993   \n",
       "3       0.036198          19.447215          2 -13800.150340 -0.118100   \n",
       "4       1.291696           0.002489          3 -14264.333939 -0.121170   \n",
       "5       1.215270           0.002882          5 -14345.202859 -0.121813   \n",
       "6       2.486759           0.002879          7 -14271.658163 -0.121214   \n",
       "7       1.934478           2.183560          1 -14454.922486 -0.123342   \n",
       "\n",
       "            MSE           MAE           SQL  \n",
       "0 -4.355624e+08 -13259.441395 -11419.164102  \n",
       "1 -4.373657e+08 -13919.888281 -11559.567488  \n",
       "2 -4.460174e+08 -13888.484726 -11730.761201  \n",
       "3 -4.905704e+08 -13800.150340 -12319.010296  \n",
       "4 -5.578932e+08 -14264.333939 -12518.047403  \n",
       "5 -5.634949e+08 -14345.202859 -12737.699989  \n",
       "6 -5.565913e+08 -14271.658163 -12750.126862  \n",
       "7 -5.651425e+08 -14454.922486 -12855.106233  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(\n",
    "    test_data,\n",
    "    extra_metrics=['MASE', 'MAPE', 'MSE', 'MAE', 'SQL'],\n",
    ")\n",
    "leaderboard.rename(columns={'score_test': 'WQL_test', 'score_val': 'WQL_val'}, inplace=True)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_mini]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n",
      "Prediction order: {'ChronosFineTuned[bolt_base]'}\n",
      "Cached predictions saved to /home/nikita/projects/time_series_analysis/code_dir/rosstat/models/auto_ml_single_target_bigger/models/cached_predictions.pkl\n"
     ]
    }
   ],
   "source": [
    "def extract_specific_rows_from_indexed_data(data, start_row: int, end_row: int):\n",
    "    rows_to_extract = np.arange(start_row, end_row)\n",
    "    unique_ids = data.index.get_level_values('item_id').unique()\n",
    "        \n",
    "    selected_data = []\n",
    "\n",
    "    for item_id in unique_ids:\n",
    "        item_data = data.loc[[item_id]]\n",
    "        \n",
    "        selected_rows = item_data.iloc[rows_to_extract]\n",
    "        selected_data.append(selected_rows)\n",
    "\n",
    "    result = pd.concat(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "k = 2\n",
    "\n",
    "top_k_models = leaderboard.sort_values(['SQL'], ascending=False).head(k)['model'].tolist()\n",
    "window_size = config.prediction_length\n",
    "test_length = test_df['code'].value_counts().iloc[0]\n",
    "max_iterations = (test_length + window_size - 1) // window_size# - 1 ещё -1 из-за known_covariates\n",
    "\n",
    "current_data = train_data.copy()\n",
    "val_predictions = {}\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    start_idx = i * window_size\n",
    "    end_idx = start_idx + window_size\n",
    "    \n",
    "    for model_name in top_k_models:\n",
    "        if model_name not in val_predictions:\n",
    "            val_predictions[model_name] = []\n",
    "        \n",
    "        # future_covariates = test_data[start_idx:start_idx + config.prediction_length][known_covariates_names]\n",
    "        # prediction_covariates = pd.concat([current_data[known_covariates_names], future_covariates])\n",
    "        \n",
    "        predictions = predictor.predict(current_data, \n",
    "                                       model=model_name,)\n",
    "                                       # known_covariates=prediction_covariates)\n",
    "                                       \n",
    "        val_predictions[model_name].append(predictions)\n",
    "        \n",
    "    current_data = pd.concat([current_data, extract_specific_rows_from_indexed_data(val_data, start_idx, end_idx)])\n",
    "\n",
    "test_df_shape = test_df.shape[0]\n",
    "val_predictions = {k: pd.concat(v)[:test_df_shape] for k, v in val_predictions.items()}\n",
    "\n",
    "current_data = val_data.copy()\n",
    "test_predictions = {}\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    start_idx = i * window_size\n",
    "    end_idx = start_idx + window_size\n",
    "    \n",
    "    for model_name in top_k_models:\n",
    "        if model_name not in test_predictions:\n",
    "            test_predictions[model_name] = []\n",
    "        \n",
    "        # future_covariates = test_data[start_idx:start_idx + config.prediction_length][known_covariates_names]\n",
    "        # prediction_covariates = pd.concat([current_data[known_covariates_names], future_covariates])\n",
    "        \n",
    "        predictions = predictor.predict(current_data, \n",
    "                                       model=model_name,)\n",
    "                                       # known_covariates=prediction_covariates)\n",
    "                                       \n",
    "        test_predictions[model_name].append(predictions)\n",
    "        \n",
    "    current_data = pd.concat([current_data, extract_specific_rows_from_indexed_data(test_data, start_idx, end_idx)])\n",
    "\n",
    "test_df_shape = test_df.shape[0]\n",
    "test_predictions = {k: pd.concat(v)[:test_df_shape] for k, v in test_predictions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fa2457e11e421f9853d73234185cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(DatePicker(value=datetime.date(2023, 1, 1), description='Start date:'), DatePick…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5b52667fbd498f80296d787057e26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import datetime\n",
    "from utils.plotting import plot_forecasts_val_test\n",
    "\n",
    "\n",
    "date_col = pd.to_datetime(test_df[\"date\"])\n",
    "min_date = date_col.min().date()\n",
    "max_date = date_col.max().date()\n",
    "size_multiplyer = 2\n",
    "height = 400 * size_multiplyer\n",
    "width = 800 * size_multiplyer * 2\n",
    "item_id = 1\n",
    "title = f'Предсказания номинальной заработной платы (для code = {item_id})'\n",
    "\n",
    "start_date_picker = widgets.DatePicker(\n",
    "    description=\"Start date:\", disabled=False, value=min_date\n",
    ")\n",
    "\n",
    "end_date_picker = widgets.DatePicker(\n",
    "    description=\"End date:\", disabled=False, value=max_date\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        start_date = datetime.datetime.combine(\n",
    "            start_date_picker.value, datetime.datetime.min.time()\n",
    "        )\n",
    "        end_date = datetime.datetime.combine(\n",
    "            end_date_picker.value, datetime.datetime.min.time()\n",
    "        )\n",
    "        plot_forecasts_val_test(\n",
    "            val_df=val_df_,\n",
    "            test_df=test_df_,\n",
    "            val_predictions=all_val_models_predictions_,\n",
    "            test_predictions=test_predictions,\n",
    "            title=title,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            item_id=item_id,\n",
    "        )\n",
    "\n",
    "\n",
    "plot_button = widgets.Button(description=\"Plot Forecasts\")\n",
    "plot_button.on_click(on_button_clicked)\n",
    "\n",
    "controls = widgets.VBox(\n",
    "    [widgets.HBox([start_date_picker, end_date_picker]), plot_button]\n",
    ")\n",
    "\n",
    "display(controls, output_area)\n",
    "\n",
    "val_df_ = val_df.rename(columns={'date': 'timestamp', \"nominal_wage\": \"target\"})[['code', 'timestamp', \"target\"]]\n",
    "val_df_ = val_df_[val_df_['code'].eq(item_id)].reset_index(drop=True)\n",
    "val_df_['timestamp'] = pd.to_datetime(val_df_['timestamp'])\n",
    "\n",
    "test_df_ = test_df.rename(columns={'date': 'timestamp', \"nominal_wage\": \"target\"})[['code', 'timestamp', \"target\"]]\n",
    "test_df_ = test_df_[test_df_['code'].eq(item_id)].reset_index(drop=True)\n",
    "test_df_['timestamp'] = pd.to_datetime(test_df_['timestamp'])\n",
    "\n",
    "val_df_ = pd.concat([val_df_, test_df_.iloc[[0]]])\n",
    "\n",
    "all_val_models_predictions_ = val_predictions.copy()\n",
    "for model_ in all_val_models_predictions_.keys():\n",
    "    all_val_models_predictions_[model_] = pd.concat([all_val_models_predictions_[model_], test_predictions[model_].loc[[item_id]].iloc[[0]]])\n",
    "\n",
    "with output_area:\n",
    "    plot_forecasts_val_test(\n",
    "        val_df=val_df_,\n",
    "        test_df=test_df_,\n",
    "        val_predictions=all_val_models_predictions_,\n",
    "        test_predictions=test_predictions,\n",
    "        title=title,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        item_id=item_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = test_df['code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ChronosFineTuned[bolt_mini]': {'MSE': 40170983.83083443,\n",
       "  'MAE': 3874.4268979279886,\n",
       "  'MAPE': 4.192300228913504,\n",
       "  'MASE': 1.2665296264320147,\n",
       "  'SQL': 1300.2013263008253},\n",
       " 'ChronosFineTuned[bolt_base]': {'MSE': 29727738.823054302,\n",
       "  'MAE': 3118.1045643682055,\n",
       "  'MAPE': 3.5263244260032187,\n",
       "  'MASE': 0.7360772940086453,\n",
       "  'SQL': 1044.5361339919227},\n",
       " 'ChronosFineTuned[bolt_small]': {'MSE': 34845768.12375621,\n",
       "  'MAE': 3483.042628132549,\n",
       "  'MAPE': 3.820555286905376,\n",
       "  'MASE': 1.012560623143062,\n",
       "  'SQL': 1161.219935477682},\n",
       " 'ChronosFineTuned[bolt_tiny]': {'MSE': 42303706.056688614,\n",
       "  'MAE': 3863.7402730600847,\n",
       "  'MAPE': 4.217355132305277,\n",
       "  'MASE': 1.151575029299694,\n",
       "  'SQL': 1282.6508712793134},\n",
       " 'ChronosZeroShot[bolt_mini]': {'MSE': 172869152.33298156,\n",
       "  'MAE': 7677.028133963618,\n",
       "  'MAPE': 8.436172229836057,\n",
       "  'MASE': 3.25467089886358,\n",
       "  'SQL': 2660.8140362381737},\n",
       " 'ChronosZeroShot[bolt_small]': {'MSE': 161446185.57887167,\n",
       "  'MAE': 7305.511725354771,\n",
       "  'MAPE': 7.982010861934878,\n",
       "  'MASE': 3.01142144421078,\n",
       "  'SQL': 2638.9355987696244},\n",
       " 'ChronosZeroShot[bolt_base]': {'MSE': 171646070.30715945,\n",
       "  'MAE': 7400.356839711655,\n",
       "  'MAPE': 8.007233537037257,\n",
       "  'MASE': 3.323825645478696,\n",
       "  'SQL': 2621.691984167421},\n",
       " 'ChronosZeroShot[bolt_tiny]': {'MSE': 193944724.8700111,\n",
       "  'MAE': 8059.607305819744,\n",
       "  'MAPE': 8.802977419272855,\n",
       "  'MASE': 2.8658366050062978,\n",
       "  'SQL': 2742.64339695929}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_metrics = {}\n",
    "\n",
    "for model in test_predictions.keys():\n",
    "    metrics_df = []\n",
    "    for code in all_codes:\n",
    "        pred_df = pd.concat([\n",
    "            test_predictions[model]\n",
    "            .loc[code][[\"0.1\", \"0.5\", \"0.9\"]]\n",
    "            .reset_index(drop=True),\n",
    "            test_df[test_df[\"code\"].eq(code)][[\"nominal_wage\"]].reset_index(drop=True),\n",
    "        ], axis=1)\n",
    "        pred_df = pd.DataFrame(pred_df)\n",
    "\n",
    "        metrics_df.append(calculate_sklearn_metrics(pred_df, target_column='nominal_wage'))\n",
    "\n",
    "    metrics_dict = pd.DataFrame(metrics_df).mean().to_dict()\n",
    "\n",
    "    all_models_metrics[model] = metrics_dict\n",
    "all_models_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run ChronosFineTuned[bolt_mini]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/150194fa061d43c2a03ae70dba7f0486\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosFineTuned[bolt_base]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/22f0838fdd7043a898145f504f720957\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosFineTuned[bolt_small]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/e1bda0ca08514c67a348a86fef742e54\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosFineTuned[bolt_tiny]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/b0d7c609430c4e4fada8a0cf8122010e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosZeroShot[bolt_mini]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/20370fab5dab409280a9f60189b46015\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosZeroShot[bolt_small]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/da31514ef8794420b378011d0830dde5\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosZeroShot[bolt_base]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/99f9e28eabc14448a9f4150c6d8a30b0\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n",
      "🏃 View run ChronosZeroShot[bolt_tiny]_AutoGluon_all_Chronos_versions at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/3be00c329be144b0b92eeaddddc70293\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n"
     ]
    }
   ],
   "source": [
    "prefix = 'AutoGluon_all_Chronos_versions'\n",
    "\n",
    "for k, metrics_ in all_models_metrics.items():\n",
    "    run_name = f\"{k}_{prefix}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_metrics(metrics_)\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "        mlflow.set_tag(\"prefix\", prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
