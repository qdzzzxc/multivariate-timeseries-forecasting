{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from utils import get_quantile_from_median, calculate_sklearn_metrics\n",
    "from xPatch_repo.exp.exp_main import Exp_Main\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"rosstat_forecasting\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data/rosstat/processed'\n",
    "\n",
    "target_column = 'nominal_wage'\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train/data.csv'))\n",
    "val_df = pd.read_csv(os.path.join(data_dir, 'val/data.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test/data.csv'))\n",
    "\n",
    "def reorder_columns_with_target_last(df, target_col):\n",
    "    columns = list(df.columns)\n",
    "    if target_col in columns:\n",
    "        columns.remove(target_col)\n",
    "        columns.append(target_col)\n",
    "    return df[columns]\n",
    "\n",
    "train_df = reorder_columns_with_target_last(train_df, target_column)\n",
    "val_df = reorder_columns_with_target_last(val_df, target_column)\n",
    "test_df = reorder_columns_with_target_last(test_df, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º\n",
    "\n",
    "scale_columns = [\n",
    "    \"nominal_wage\",\n",
    "    \"capital_labor_ratio_change\",\n",
    "    \"capital_productivity_change\",\n",
    "    \"fixed_assets_renewal_comparable_prices\",\n",
    "    \"labor_productivity\",\n",
    "    \"high_productivity_jobs\",\n",
    "    \"machinery_share_in_total_assets\",\n",
    "    \"investment_share_for_modernization\",\n",
    "    \"production_index_yoy\",\n",
    "    \"production_index_mom\",\n",
    "]\n",
    "os.makedirs(\"./artifacts\", exist_ok=True)\n",
    "\n",
    "train_scaled = train_df.copy()\n",
    "val_scaled = val_df.copy()\n",
    "test_scaled = test_df.copy()\n",
    "\n",
    "scalers = {}\n",
    "for column in scale_columns:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    train_scaled[column] = scaler.fit_transform(train_df[[column]])\n",
    "    val_scaled[column] = scaler.transform(val_df[[column]])\n",
    "    test_scaled[column] = scaler.transform(test_df[[column]])\n",
    "\n",
    "    scalers[column] = scaler\n",
    "    joblib.dump(scaler, f\"./artifacts/scaler_{column}.joblib\")\n",
    "\n",
    "joblib.dump(scalers, \"./artifacts/all_column_scalers.joblib\")\n",
    "\n",
    "concated_data_path = os.path.join(data_dir, 'xPatch_concated_data.csv')\n",
    "\n",
    "train_val_df = pd.concat([train_scaled, val_scaled])\n",
    "train_val_df.to_csv(concated_data_path, index=False)\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'xPatch_test_data.csv')\n",
    "pd.concat([val_scaled, test_scaled]).to_csv(test_data_path, index=False)\n",
    "\n",
    "val_data_path = os.path.join(data_dir, 'xPatch_val_data.csv')\n",
    "pd.concat([train_scaled.groupby(by=['code']).tail(12), val_scaled]).to_csv(val_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seq_len = 12\n",
    "        self.label_len = 12\n",
    "        self.pred_len = 2\n",
    "        \n",
    "        self.is_training = 1\n",
    "        self.model_id = 'rosstat'\n",
    "        self.model = 'xPatch'\n",
    "        self.data = 'custom_multi'\n",
    "        \n",
    "        self.root_path = data_dir\n",
    "        self.data_path = 'xPatch_concated_data.csv'\n",
    "        \n",
    "        self.features = 'MS'\n",
    "        self.target = 'nominal_wage'\n",
    "        self.freq = 'M'\n",
    "        self.scale = False\n",
    "        self.timeenc = 0\n",
    "        self.train_only = False\n",
    "        \n",
    "        self.enc_in = 10\n",
    "        self.embed = 'timeF'\n",
    "        \n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.padding_patch = 'end'\n",
    "        \n",
    "        self.ma_type = 'ema'\n",
    "        self.alpha = 0.3\n",
    "        self.beta = 0.3\n",
    "        \n",
    "        self.batch_size = 8\n",
    "        self.train_epochs = 100\n",
    "        self.patience = 10\n",
    "        self.learning_rate = 5e-3\n",
    "        self.loss = 'mse'\n",
    "        self.lradj = 'type1'\n",
    "        self.use_amp = False\n",
    "        self.revin = 1\n",
    "        \n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "        self.gpu = 0\n",
    "        self.use_multi_gpu = False\n",
    "        self.devices = '0'\n",
    "        self.test_flop = False\n",
    "        \n",
    "        self.checkpoints = './checkpoints/'\n",
    "        self.des = 'test'\n",
    "        self.itr = 1\n",
    "        self.num_workers = 10\n",
    "\n",
    "        self.train_perc = 0.95\n",
    "        self.test_perc = 0.0\n",
    "\n",
    "args = Args()\n",
    "assert not args.scale, '–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å –≤—Ä—É—á–Ω—É—é'\n",
    "experiment = Exp_Main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3795\n",
      "val 207\n",
      "\titers: 100, epoch: 1 | loss: 0.1189000\n",
      "\tspeed: 0.0047s/iter; left time: 223.7092s\n",
      "\titers: 200, epoch: 1 | loss: 0.1182361\n",
      "\tspeed: 0.0030s/iter; left time: 142.3323s\n",
      "\titers: 300, epoch: 1 | loss: 0.1758851\n",
      "\tspeed: 0.0031s/iter; left time: 147.5759s\n",
      "\titers: 400, epoch: 1 | loss: 0.0525256\n",
      "\tspeed: 0.0032s/iter; left time: 150.1644s\n",
      "Epoch: 1 cost time: 1.6613800525665283\n",
      "Epoch: 1, Steps: 474 | Train Loss: 0.1141962 Vali Loss: 0.1646948 Test Loss: 0.0000000\n",
      "Validation loss decreased (inf --> 0.164695).  Saving model ...\n",
      "Updating learning rate to 0.005\n",
      "\titers: 100, epoch: 2 | loss: 0.0844330\n",
      "\tspeed: 0.0094s/iter; left time: 440.2887s\n",
      "\titers: 200, epoch: 2 | loss: 0.1325845\n",
      "\tspeed: 0.0030s/iter; left time: 140.2346s\n",
      "\titers: 300, epoch: 2 | loss: 0.0625288\n",
      "\tspeed: 0.0031s/iter; left time: 142.9997s\n",
      "\titers: 400, epoch: 2 | loss: 0.0834241\n",
      "\tspeed: 0.0030s/iter; left time: 138.6782s\n",
      "Epoch: 2 cost time: 1.612511157989502\n",
      "Epoch: 2, Steps: 474 | Train Loss: 0.0988746 Vali Loss: 0.0968180 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.164695 --> 0.096818).  Saving model ...\n",
      "Updating learning rate to 0.0025\n",
      "\titers: 100, epoch: 3 | loss: 0.0632371\n",
      "\tspeed: 0.0092s/iter; left time: 426.9216s\n",
      "\titers: 200, epoch: 3 | loss: 0.0905056\n",
      "\tspeed: 0.0032s/iter; left time: 145.8428s\n",
      "\titers: 300, epoch: 3 | loss: 0.0760001\n",
      "\tspeed: 0.0031s/iter; left time: 141.9522s\n",
      "\titers: 400, epoch: 3 | loss: 0.0980806\n",
      "\tspeed: 0.0030s/iter; left time: 138.9478s\n",
      "Epoch: 3 cost time: 1.6132478713989258\n",
      "Epoch: 3, Steps: 474 | Train Loss: 0.0874906 Vali Loss: 0.0898453 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.096818 --> 0.089845).  Saving model ...\n",
      "Updating learning rate to 0.00125\n",
      "\titers: 100, epoch: 4 | loss: 0.0605680\n",
      "\tspeed: 0.0091s/iter; left time: 416.9413s\n",
      "\titers: 200, epoch: 4 | loss: 0.1379586\n",
      "\tspeed: 0.0032s/iter; left time: 144.6909s\n",
      "\titers: 300, epoch: 4 | loss: 0.0839583\n",
      "\tspeed: 0.0032s/iter; left time: 146.0000s\n",
      "\titers: 400, epoch: 4 | loss: 0.1466328\n",
      "\tspeed: 0.0030s/iter; left time: 138.7006s\n",
      "Epoch: 4 cost time: 1.642085075378418\n",
      "Epoch: 4, Steps: 474 | Train Loss: 0.0818596 Vali Loss: 0.0905976 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.000625\n",
      "\titers: 100, epoch: 5 | loss: 0.0497497\n",
      "\tspeed: 0.0094s/iter; left time: 428.0755s\n",
      "\titers: 200, epoch: 5 | loss: 0.0945655\n",
      "\tspeed: 0.0030s/iter; left time: 134.8788s\n",
      "\titers: 300, epoch: 5 | loss: 0.0452550\n",
      "\tspeed: 0.0030s/iter; left time: 134.7616s\n",
      "\titers: 400, epoch: 5 | loss: 0.0317013\n",
      "\tspeed: 0.0031s/iter; left time: 139.4529s\n",
      "Epoch: 5 cost time: 1.6201441287994385\n",
      "Epoch: 5, Steps: 474 | Train Loss: 0.0792149 Vali Loss: 0.0900113 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 0.0003125\n",
      "\titers: 100, epoch: 6 | loss: 0.0724212\n",
      "\tspeed: 0.0094s/iter; left time: 421.7410s\n",
      "\titers: 200, epoch: 6 | loss: 0.1463383\n",
      "\tspeed: 0.0032s/iter; left time: 142.2739s\n",
      "\titers: 300, epoch: 6 | loss: 0.0470987\n",
      "\tspeed: 0.0031s/iter; left time: 139.3173s\n",
      "\titers: 400, epoch: 6 | loss: 0.1469142\n",
      "\tspeed: 0.0031s/iter; left time: 137.8016s\n",
      "Epoch: 6 cost time: 1.6548597812652588\n",
      "Epoch: 6, Steps: 474 | Train Loss: 0.0777683 Vali Loss: 0.0911378 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 0.00015625\n",
      "\titers: 100, epoch: 7 | loss: 0.1331242\n",
      "\tspeed: 0.0095s/iter; left time: 423.4768s\n",
      "\titers: 200, epoch: 7 | loss: 0.0402789\n",
      "\tspeed: 0.0031s/iter; left time: 136.3177s\n",
      "\titers: 300, epoch: 7 | loss: 0.0540025\n",
      "\tspeed: 0.0030s/iter; left time: 131.3568s\n",
      "\titers: 400, epoch: 7 | loss: 0.0437547\n",
      "\tspeed: 0.0030s/iter; left time: 133.8274s\n",
      "Epoch: 7 cost time: 1.6381735801696777\n",
      "Epoch: 7, Steps: 474 | Train Loss: 0.0766626 Vali Loss: 0.0933600 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.8125e-05\n",
      "\titers: 100, epoch: 8 | loss: 0.0394918\n",
      "\tspeed: 0.0093s/iter; left time: 409.1756s\n",
      "\titers: 200, epoch: 8 | loss: 0.1331128\n",
      "\tspeed: 0.0030s/iter; left time: 130.0750s\n",
      "\titers: 300, epoch: 8 | loss: 0.1086010\n",
      "\tspeed: 0.0030s/iter; left time: 132.5286s\n",
      "\titers: 400, epoch: 8 | loss: 0.0541681\n",
      "\tspeed: 0.0030s/iter; left time: 132.9696s\n",
      "Epoch: 8 cost time: 1.6095407009124756\n",
      "Epoch: 8, Steps: 474 | Train Loss: 0.0763647 Vali Loss: 0.0922118 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.90625e-05\n",
      "\titers: 100, epoch: 9 | loss: 0.0728747\n",
      "\tspeed: 0.0094s/iter; left time: 410.2882s\n",
      "\titers: 200, epoch: 9 | loss: 0.0723469\n",
      "\tspeed: 0.0029s/iter; left time: 124.3612s\n",
      "\titers: 300, epoch: 9 | loss: 0.0467037\n",
      "\tspeed: 0.0030s/iter; left time: 128.7130s\n",
      "\titers: 400, epoch: 9 | loss: 0.0571679\n",
      "\tspeed: 0.0031s/iter; left time: 132.2168s\n",
      "Epoch: 9 cost time: 1.60433030128479\n",
      "Epoch: 9, Steps: 474 | Train Loss: 0.0760262 Vali Loss: 0.0931724 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.953125e-05\n",
      "\titers: 100, epoch: 10 | loss: 0.0820089\n",
      "\tspeed: 0.0094s/iter; left time: 403.2003s\n",
      "\titers: 200, epoch: 10 | loss: 0.0907213\n",
      "\tspeed: 0.0030s/iter; left time: 130.5125s\n",
      "\titers: 300, epoch: 10 | loss: 0.1149156\n",
      "\tspeed: 0.0032s/iter; left time: 135.2045s\n",
      "\titers: 400, epoch: 10 | loss: 0.0540499\n",
      "\tspeed: 0.0032s/iter; left time: 134.9613s\n",
      "Epoch: 10 cost time: 1.652482271194458\n",
      "Epoch: 10, Steps: 474 | Train Loss: 0.0758364 Vali Loss: 0.0907325 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 9.765625e-06\n",
      "\titers: 100, epoch: 11 | loss: 0.0914303\n",
      "\tspeed: 0.0095s/iter; left time: 403.8748s\n",
      "\titers: 200, epoch: 11 | loss: 0.1055477\n",
      "\tspeed: 0.0031s/iter; left time: 130.7609s\n",
      "\titers: 300, epoch: 11 | loss: 0.0462133\n",
      "\tspeed: 0.0030s/iter; left time: 127.0461s\n",
      "\titers: 400, epoch: 11 | loss: 0.0483316\n",
      "\tspeed: 0.0030s/iter; left time: 128.4885s\n",
      "Epoch: 11 cost time: 1.6373484134674072\n",
      "Epoch: 11, Steps: 474 | Train Loss: 0.0757917 Vali Loss: 0.0922560 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.8828125e-06\n",
      "\titers: 100, epoch: 12 | loss: 0.0394179\n",
      "\tspeed: 0.0094s/iter; left time: 394.7470s\n",
      "\titers: 200, epoch: 12 | loss: 0.0579420\n",
      "\tspeed: 0.0030s/iter; left time: 126.5319s\n",
      "\titers: 300, epoch: 12 | loss: 0.0547427\n",
      "\tspeed: 0.0030s/iter; left time: 127.6251s\n",
      "\titers: 400, epoch: 12 | loss: 0.1053321\n",
      "\tspeed: 0.0031s/iter; left time: 129.4879s\n",
      "Epoch: 12 cost time: 1.6273198127746582\n",
      "Epoch: 12, Steps: 474 | Train Loss: 0.0758172 Vali Loss: 0.0938873 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.44140625e-06\n",
      "\titers: 100, epoch: 13 | loss: 0.0551001\n",
      "\tspeed: 0.0096s/iter; left time: 399.6144s\n",
      "\titers: 200, epoch: 13 | loss: 0.0600886\n",
      "\tspeed: 0.0030s/iter; left time: 123.6791s\n",
      "\titers: 300, epoch: 13 | loss: 0.0631276\n",
      "\tspeed: 0.0030s/iter; left time: 123.5125s\n",
      "\titers: 400, epoch: 13 | loss: 0.0530902\n",
      "\tspeed: 0.0030s/iter; left time: 125.1240s\n",
      "Epoch: 13 cost time: 1.6326022148132324\n",
      "Epoch: 13, Steps: 474 | Train Loss: 0.0759074 Vali Loss: 0.0896574 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.089845 --> 0.089657).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-06\n",
      "\titers: 100, epoch: 14 | loss: 0.0717209\n",
      "\tspeed: 0.0093s/iter; left time: 383.8744s\n",
      "\titers: 200, epoch: 14 | loss: 0.0666857\n",
      "\tspeed: 0.0030s/iter; left time: 124.5392s\n",
      "\titers: 300, epoch: 14 | loss: 0.0910633\n",
      "\tspeed: 0.0030s/iter; left time: 122.6796s\n",
      "\titers: 400, epoch: 14 | loss: 0.0509803\n",
      "\tspeed: 0.0031s/iter; left time: 126.7064s\n",
      "Epoch: 14 cost time: 1.6328363418579102\n",
      "Epoch: 14, Steps: 474 | Train Loss: 0.0759247 Vali Loss: 0.0919023 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.103515625e-07\n",
      "\titers: 100, epoch: 15 | loss: 0.0874283\n",
      "\tspeed: 0.0095s/iter; left time: 388.3029s\n",
      "\titers: 200, epoch: 15 | loss: 0.0556529\n",
      "\tspeed: 0.0031s/iter; left time: 123.7486s\n",
      "\titers: 300, epoch: 15 | loss: 0.0610241\n",
      "\tspeed: 0.0030s/iter; left time: 120.2199s\n",
      "\titers: 400, epoch: 15 | loss: 0.0682061\n",
      "\tspeed: 0.0030s/iter; left time: 122.0195s\n",
      "Epoch: 15 cost time: 1.6199216842651367\n",
      "Epoch: 15, Steps: 474 | Train Loss: 0.0759061 Vali Loss: 0.0925635 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.0517578125e-07\n",
      "\titers: 100, epoch: 16 | loss: 0.0509862\n",
      "\tspeed: 0.0093s/iter; left time: 375.3577s\n",
      "\titers: 200, epoch: 16 | loss: 0.0567554\n",
      "\tspeed: 0.0030s/iter; left time: 120.7751s\n",
      "\titers: 300, epoch: 16 | loss: 0.1104145\n",
      "\tspeed: 0.0032s/iter; left time: 127.0428s\n",
      "\titers: 400, epoch: 16 | loss: 0.0533204\n",
      "\tspeed: 0.0032s/iter; left time: 126.3896s\n",
      "Epoch: 16 cost time: 1.6497445106506348\n",
      "Epoch: 16, Steps: 474 | Train Loss: 0.0760536 Vali Loss: 0.0921240 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.52587890625e-07\n",
      "\titers: 100, epoch: 17 | loss: 0.0447833\n",
      "\tspeed: 0.0096s/iter; left time: 381.2962s\n",
      "\titers: 200, epoch: 17 | loss: 0.0868127\n",
      "\tspeed: 0.0030s/iter; left time: 119.7207s\n",
      "\titers: 300, epoch: 17 | loss: 0.0886425\n",
      "\tspeed: 0.0030s/iter; left time: 118.5277s\n",
      "\titers: 400, epoch: 17 | loss: 0.0569823\n",
      "\tspeed: 0.0030s/iter; left time: 119.4854s\n",
      "Epoch: 17 cost time: 1.6379878520965576\n",
      "Epoch: 17, Steps: 474 | Train Loss: 0.0760489 Vali Loss: 0.0922883 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.62939453125e-08\n",
      "\titers: 100, epoch: 18 | loss: 0.0476896\n",
      "\tspeed: 0.0095s/iter; left time: 373.9729s\n",
      "\titers: 200, epoch: 18 | loss: 0.1210445\n",
      "\tspeed: 0.0031s/iter; left time: 119.9501s\n",
      "\titers: 300, epoch: 18 | loss: 0.0493025\n",
      "\tspeed: 0.0030s/iter; left time: 117.3543s\n",
      "\titers: 400, epoch: 18 | loss: 0.0664250\n",
      "\tspeed: 0.0030s/iter; left time: 116.9285s\n",
      "Epoch: 18 cost time: 1.6247665882110596\n",
      "Epoch: 18, Steps: 474 | Train Loss: 0.0758159 Vali Loss: 0.0940915 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 3.814697265625e-08\n",
      "\titers: 100, epoch: 19 | loss: 0.0268871\n",
      "\tspeed: 0.0097s/iter; left time: 374.3721s\n",
      "\titers: 200, epoch: 19 | loss: 0.0563391\n",
      "\tspeed: 0.0031s/iter; left time: 119.6093s\n",
      "\titers: 300, epoch: 19 | loss: 0.0802767\n",
      "\tspeed: 0.0030s/iter; left time: 116.4957s\n",
      "\titers: 400, epoch: 19 | loss: 0.1010649\n",
      "\tspeed: 0.0031s/iter; left time: 117.4448s\n",
      "Epoch: 19 cost time: 1.6662514209747314\n",
      "Epoch: 19, Steps: 474 | Train Loss: 0.0757832 Vali Loss: 0.0916904 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 1.9073486328125e-08\n",
      "\titers: 100, epoch: 20 | loss: 0.0663252\n",
      "\tspeed: 0.0095s/iter; left time: 365.3003s\n",
      "\titers: 200, epoch: 20 | loss: 0.0611781\n",
      "\tspeed: 0.0030s/iter; left time: 115.8273s\n",
      "\titers: 300, epoch: 20 | loss: 0.0361218\n",
      "\tspeed: 0.0031s/iter; left time: 118.1929s\n",
      "\titers: 400, epoch: 20 | loss: 0.1414264\n",
      "\tspeed: 0.0031s/iter; left time: 119.6559s\n",
      "Epoch: 20 cost time: 1.648585557937622\n",
      "Epoch: 20, Steps: 474 | Train Loss: 0.0762079 Vali Loss: 0.0934149 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 9.5367431640625e-09\n",
      "\titers: 100, epoch: 21 | loss: 0.0681717\n",
      "\tspeed: 0.0101s/iter; left time: 380.6424s\n",
      "\titers: 200, epoch: 21 | loss: 0.0274355\n",
      "\tspeed: 0.0031s/iter; left time: 115.2130s\n",
      "\titers: 300, epoch: 21 | loss: 0.1011985\n",
      "\tspeed: 0.0031s/iter; left time: 116.7454s\n",
      "\titers: 400, epoch: 21 | loss: 0.0422702\n",
      "\tspeed: 0.0031s/iter; left time: 114.6216s\n",
      "Epoch: 21 cost time: 1.6465685367584229\n",
      "Epoch: 21, Steps: 474 | Train Loss: 0.0758159 Vali Loss: 0.0933424 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 4.76837158203125e-09\n",
      "\titers: 100, epoch: 22 | loss: 0.0719740\n",
      "\tspeed: 0.0094s/iter; left time: 350.9675s\n",
      "\titers: 200, epoch: 22 | loss: 0.0887875\n",
      "\tspeed: 0.0030s/iter; left time: 110.1056s\n",
      "\titers: 300, epoch: 22 | loss: 0.0504555\n",
      "\tspeed: 0.0031s/iter; left time: 113.5635s\n",
      "\titers: 400, epoch: 22 | loss: 0.0308681\n",
      "\tspeed: 0.0033s/iter; left time: 120.4597s\n",
      "Epoch: 22 cost time: 1.6597957611083984\n",
      "Epoch: 22, Steps: 474 | Train Loss: 0.0758938 Vali Loss: 0.0919190 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 2.384185791015625e-09\n",
      "\titers: 100, epoch: 23 | loss: 0.0576605\n",
      "\tspeed: 0.0099s/iter; left time: 363.8420s\n",
      "\titers: 200, epoch: 23 | loss: 0.0389026\n",
      "\tspeed: 0.0031s/iter; left time: 115.2278s\n",
      "\titers: 300, epoch: 23 | loss: 0.0357366\n",
      "\tspeed: 0.0032s/iter; left time: 118.6251s\n",
      "\titers: 400, epoch: 23 | loss: 0.0514801\n",
      "\tspeed: 0.0032s/iter; left time: 116.5305s\n",
      "Epoch: 23 cost time: 1.6865665912628174\n",
      "Epoch: 23, Steps: 474 | Train Loss: 0.0757389 Vali Loss: 0.0925830 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.des, ii)\n",
    "\n",
    "model = experiment.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –Ω–∞—Å—Ç–æ—è—â–µ–º test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69, 11)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "\n",
    "val_args = copy(args)\n",
    "\n",
    "val_args.train_perc = 1.\n",
    "val_args.train_only = True\n",
    "val_args.data_path = 'xPatch_val_data.csv'\n",
    "experiment = Exp_Main(val_args)\n",
    "\n",
    "val_dataset, _ = experiment._get_data(flag='train')\n",
    "true_val_datasets = val_dataset.datasets\n",
    "len(true_val_datasets), len(true_val_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69, 11)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_args = copy(args)\n",
    "\n",
    "test_args.train_perc = 1.\n",
    "test_args.train_only = True\n",
    "test_args.data_path = 'xPatch_test_data.csv'\n",
    "experiment = Exp_Main(val_args)\n",
    "\n",
    "test_dataset, _ = experiment._get_data(flag='train')\n",
    "true_test_datasets = test_dataset.datasets\n",
    "len(true_test_datasets), len(true_test_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "val_predictions = {}\n",
    "val_predictions_df = pd.DataFrame()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "all_codes = train_df['code'].unique()\n",
    "\n",
    "for code, true_val_dataset in zip(all_codes, true_val_datasets):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    for i in tqdm(range(0, len(true_val_dataset), args.pred_len), disable=True):\n",
    "        batch_x, batch_y, _, _ = true_val_dataset[i]\n",
    "        batch_x = torch.tensor(batch_x).unsqueeze(0).float().to(device)\n",
    "        batch_y = torch.tensor(batch_y).unsqueeze(0).float().to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch_x)\n",
    "\n",
    "        y_pred = scalers['nominal_wage'].inverse_transform(preds.detach().squeeze(0).cpu().numpy())[..., -1:].flatten()\n",
    "        y_true = scalers['nominal_wage'].inverse_transform(batch_y.detach().squeeze(0).cpu().numpy())[:preds.shape[1], -1:].flatten()\n",
    "\n",
    "        all_preds.append(y_pred)\n",
    "        all_true.append(y_true)\n",
    "\n",
    "    predictions = np.concatenate(all_preds)\n",
    "    true_values = np.concatenate(all_true)\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "                    predictions,\n",
    "                    true_values,\n",
    "                ]).transpose()\n",
    "    \n",
    "    df.columns = ['mean', 'y_true']\n",
    "    df['code'] = code\n",
    "    df['0.1'] = get_quantile_from_median(df['mean'].values, 0.1)\n",
    "    df['0.9'] = get_quantile_from_median(df['mean'].values, 0.9)\n",
    "\n",
    "    val_predictions_df = pd.concat([val_predictions_df, df])\n",
    "\n",
    "val_predictions['xPatch'] = val_predictions_df.set_index('code')\n",
    "\n",
    "\n",
    "test_predictions = {}\n",
    "test_predictions_df = pd.DataFrame()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "all_codes = train_df['code'].unique()\n",
    "\n",
    "for code, true_test_dataset in zip(all_codes, true_test_datasets):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    for i in tqdm(range(0, len(true_test_dataset), args.pred_len), disable=True):\n",
    "        batch_x, batch_y, _, _ = true_test_dataset[i]\n",
    "        batch_x = torch.tensor(batch_x).unsqueeze(0).float().to(device)\n",
    "        batch_y = torch.tensor(batch_y).unsqueeze(0).float().to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch_x)\n",
    "\n",
    "        y_pred = scalers['nominal_wage'].inverse_transform(preds.detach().squeeze(0).cpu().numpy())[..., -1:].flatten()\n",
    "        y_true = scalers['nominal_wage'].inverse_transform(batch_y.detach().squeeze(0).cpu().numpy())[:preds.shape[1], -1:].flatten()\n",
    "\n",
    "        all_preds.append(y_pred)\n",
    "        all_true.append(y_true)\n",
    "\n",
    "    predictions = np.concatenate(all_preds)\n",
    "    true_values = np.concatenate(all_true)\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "                    predictions,\n",
    "                    true_values,\n",
    "                ]).transpose()\n",
    "    \n",
    "    df.columns = ['mean', 'y_true']\n",
    "    df['code'] = code\n",
    "    df['0.1'] = get_quantile_from_median(df['mean'].values, 0.1)\n",
    "    df['0.9'] = get_quantile_from_median(df['mean'].values, 0.9)\n",
    "\n",
    "    test_predictions_df = pd.concat([test_predictions_df, df])\n",
    "\n",
    "test_predictions['xPatch'] = test_predictions_df.set_index('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b4742ce2204e0981f07f6755ffe0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(DatePicker(value=datetime.date(2023, 1, 1), description='Start date:'), DatePick‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1069c2f8316547808326bdf792126a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import datetime\n",
    "from utils.plotting import plot_forecasts_val_test\n",
    "\n",
    "\n",
    "date_col = pd.to_datetime(test_df[\"date\"])\n",
    "min_date = date_col.min().date()\n",
    "max_date = date_col.max().date()\n",
    "size_multiplyer = 2\n",
    "height = 400 * size_multiplyer\n",
    "width = 800 * size_multiplyer\n",
    "item_id = 1\n",
    "title = f'–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–æ–º–∏–Ω–∞–ª—å–Ω–æ–π –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã (–¥–ª—è code = {item_id})'\n",
    "\n",
    "start_date_picker = widgets.DatePicker(\n",
    "    description=\"Start date:\", disabled=False, value=min_date\n",
    ")\n",
    "\n",
    "end_date_picker = widgets.DatePicker(\n",
    "    description=\"End date:\", disabled=False, value=max_date\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        start_date = datetime.datetime.combine(\n",
    "            start_date_picker.value, datetime.datetime.min.time()\n",
    "        )\n",
    "        end_date = datetime.datetime.combine(\n",
    "            end_date_picker.value, datetime.datetime.min.time()\n",
    "        )\n",
    "        plot_forecasts_val_test(\n",
    "            val_df=val_df_,\n",
    "            test_df=test_df_,\n",
    "            val_predictions=all_val_models_predictions_,\n",
    "            test_predictions=test_predictions,\n",
    "            title=title,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            item_id=item_id,\n",
    "        )\n",
    "\n",
    "\n",
    "plot_button = widgets.Button(description=\"Plot Forecasts\")\n",
    "plot_button.on_click(on_button_clicked)\n",
    "\n",
    "controls = widgets.VBox(\n",
    "    [widgets.HBox([start_date_picker, end_date_picker]), plot_button]\n",
    ")\n",
    "\n",
    "display(controls, output_area)\n",
    "\n",
    "val_df_ = val_df.rename(columns={'date': 'timestamp', \"nominal_wage\": \"target\"})[['code', 'timestamp', \"target\"]]\n",
    "val_df_ = val_df_[val_df_['code'].eq(item_id)].reset_index(drop=True)\n",
    "val_df_['timestamp'] = pd.to_datetime(val_df_['timestamp'])\n",
    "\n",
    "test_df_ = test_df.rename(columns={'date': 'timestamp', \"nominal_wage\": \"target\"})[['code', 'timestamp', \"target\"]]\n",
    "test_df_ = test_df_[test_df_['code'].eq(item_id)].reset_index(drop=True)\n",
    "test_df_['timestamp'] = pd.to_datetime(test_df_['timestamp'])\n",
    "\n",
    "val_df_ = pd.concat([val_df_, test_df_.iloc[[0]]])\n",
    "\n",
    "all_val_models_predictions_ = val_predictions.copy()\n",
    "for model_ in all_val_models_predictions_.keys():\n",
    "    all_val_models_predictions_[model_] = pd.concat([all_val_models_predictions_[model_], test_predictions[model_].loc[[item_id]].iloc[[0]]])\n",
    "\n",
    "with output_area:\n",
    "    plot_forecasts_val_test(\n",
    "        val_df=val_df_,\n",
    "        test_df=test_df_,\n",
    "        val_predictions=all_val_models_predictions_,\n",
    "        test_predictions=test_predictions,\n",
    "        title=title,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        item_id=item_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xPatch': {'MSE': 172628571.59412074,\n",
       "  'MAE': 11447.651473335598,\n",
       "  'MAPE': 14.688398540614227,\n",
       "  'MASE': 3.4372183596557337,\n",
       "  'SQL': 3415.250993703453}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_metrics = {}\n",
    "\n",
    "test_predictions\n",
    "for model in test_predictions.keys():\n",
    "    metrics_df = []\n",
    "    for code in all_codes:\n",
    "        pred_df = pd.concat([\n",
    "            test_predictions[model].rename(columns={'mean': '0.5'})\n",
    "            .loc[code][[\"0.1\", \"0.5\", \"0.9\"]]\n",
    "            .reset_index(drop=True),\n",
    "            test_df[test_df[\"code\"].eq(code)][[\"nominal_wage\"]].reset_index(drop=True),\n",
    "        ], axis=1)\n",
    "        pred_df = pd.DataFrame(pred_df)\n",
    "\n",
    "        metrics_df.append(calculate_sklearn_metrics(pred_df, target_column='nominal_wage'))\n",
    "\n",
    "    metrics_dict = pd.DataFrame(metrics_df).mean().to_dict()\n",
    "\n",
    "    all_models_metrics[model] = metrics_dict\n",
    "\n",
    "all_models_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run xPatch_xPatch at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/8336b198560a4747bf9a7f81c569cda8\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n"
     ]
    }
   ],
   "source": [
    "prefix = 'xPatch'\n",
    "\n",
    "for k, metrics_ in all_models_metrics.items():\n",
    "    run_name = f\"{k}_{prefix}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_metrics(metrics_)\n",
    "        mlflow.log_param(\"model_name\", k)\n",
    "\n",
    "        mlflow.set_tag(\"prefix\", prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
