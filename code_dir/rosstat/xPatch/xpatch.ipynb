{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from utils import get_quantile_from_median, calculate_sklearn_metrics\n",
    "from xPatch_repo.exp.exp_main import Exp_Main\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"rosstat_forecasting\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data/rosstat/processed'\n",
    "\n",
    "target_column = 'nominal_wage'\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train/data.csv'))\n",
    "val_df = pd.read_csv(os.path.join(data_dir, 'val/data.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test/data.csv'))\n",
    "\n",
    "def reorder_columns_with_target_last(df, target_col):\n",
    "    columns = list(df.columns)\n",
    "    if target_col in columns:\n",
    "        columns.remove(target_col)\n",
    "        columns.append(target_col)\n",
    "    return df[columns]\n",
    "\n",
    "train_df = reorder_columns_with_target_last(train_df, target_column)\n",
    "val_df = reorder_columns_with_target_last(val_df, target_column)\n",
    "test_df = reorder_columns_with_target_last(test_df, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация по колонкам\n",
    "\n",
    "scale_columns = [\n",
    "    \"nominal_wage\",\n",
    "    \"capital_labor_ratio_change\",\n",
    "    \"capital_productivity_change\",\n",
    "    \"fixed_assets_renewal_comparable_prices\",\n",
    "    \"labor_productivity\",\n",
    "    \"high_productivity_jobs\",\n",
    "    \"machinery_share_in_total_assets\",\n",
    "    \"investment_share_for_modernization\",\n",
    "    \"production_index_yoy\",\n",
    "    \"production_index_mom\",\n",
    "]\n",
    "os.makedirs(\"./artifacts\", exist_ok=True)\n",
    "\n",
    "train_scaled = train_df.copy()\n",
    "val_scaled = val_df.copy()\n",
    "test_scaled = test_df.copy()\n",
    "\n",
    "scalers = {}\n",
    "for column in scale_columns:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    train_scaled[column] = scaler.fit_transform(train_df[[column]])\n",
    "    val_scaled[column] = scaler.transform(val_df[[column]])\n",
    "    test_scaled[column] = scaler.transform(test_df[[column]])\n",
    "\n",
    "    scalers[column] = scaler\n",
    "    joblib.dump(scaler, f\"./artifacts/scaler_{column}.joblib\")\n",
    "\n",
    "joblib.dump(scalers, \"./artifacts/all_column_scalers.joblib\")\n",
    "\n",
    "concated_data_path = os.path.join(data_dir, 'xPatch_concated_data.csv')\n",
    "\n",
    "train_val_df = pd.concat([train_scaled, val_scaled])\n",
    "train_val_df.to_csv(concated_data_path, index=False)\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'xPatch_test_data.csv')\n",
    "pd.concat([val_scaled, test_scaled]).to_csv(test_data_path, index=False)\n",
    "\n",
    "test_data_path = os.path.join(data_dir, 'xPatch_val_data.csv')\n",
    "pd.concat([train_scaled.groupby(by=['code']).head(12), val_scaled]).to_csv(test_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seq_len = 12\n",
    "        self.label_len = 12\n",
    "        self.pred_len = 2\n",
    "        \n",
    "        self.is_training = 1\n",
    "        self.model_id = 'rosstat'\n",
    "        self.model = 'xPatch'\n",
    "        self.data = 'custom_multi'\n",
    "        \n",
    "        self.root_path = data_dir\n",
    "        self.data_path = 'xPatch_concated_data.csv'\n",
    "        \n",
    "        self.features = 'MS'\n",
    "        self.target = 'nominal_wage'\n",
    "        self.freq = 'M'\n",
    "        self.scale = False\n",
    "        self.timeenc = 0\n",
    "        self.train_only = False\n",
    "        \n",
    "        self.enc_in = 10\n",
    "        self.embed = 'timeF'\n",
    "        \n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.padding_patch = 'end'\n",
    "        \n",
    "        self.ma_type = 'ema'\n",
    "        self.alpha = 0.3\n",
    "        self.beta = 0.3\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.train_epochs = 100\n",
    "        self.patience = 10\n",
    "        self.learning_rate = 1e-4\n",
    "        self.loss = 'mse'\n",
    "        self.lradj = 'type1'\n",
    "        self.use_amp = False\n",
    "        self.revin = 1\n",
    "        \n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "        self.gpu = 0\n",
    "        self.use_multi_gpu = False\n",
    "        self.devices = '0'\n",
    "        self.test_flop = False\n",
    "        \n",
    "        self.checkpoints = './checkpoints/'\n",
    "        self.des = 'test'\n",
    "        self.itr = 1\n",
    "        self.num_workers = 10\n",
    "\n",
    "        self.train_perc = 0.95\n",
    "        self.test_perc = 0.0\n",
    "\n",
    "args = Args()\n",
    "assert not args.scale, 'Нормализацию нужно проводить вручную'\n",
    "experiment = Exp_Main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 3795\n",
      "val 207\n",
      "\titers: 100, epoch: 1 | loss: 0.1524249\n",
      "\tspeed: 0.0058s/iter; left time: 67.5398s\n",
      "Epoch: 1 cost time: 0.6663775444030762\n",
      "Epoch: 1, Steps: 118 | Train Loss: 0.1718119 Vali Loss: 0.2235939 Test Loss: 0.0000000\n",
      "Validation loss decreased (inf --> 0.223594).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.1126798\n",
      "\tspeed: 0.0092s/iter; left time: 106.1968s\n",
      "Epoch: 2 cost time: 0.6499958038330078\n",
      "Epoch: 2, Steps: 118 | Train Loss: 0.1546323 Vali Loss: 0.1977078 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.223594 --> 0.197708).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1243683\n",
      "\tspeed: 0.0094s/iter; left time: 108.0744s\n",
      "Epoch: 3 cost time: 0.6318421363830566\n",
      "Epoch: 3, Steps: 118 | Train Loss: 0.1433684 Vali Loss: 0.1904631 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.197708 --> 0.190463).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.1800199\n",
      "\tspeed: 0.0088s/iter; left time: 100.3891s\n",
      "Epoch: 4 cost time: 0.6300041675567627\n",
      "Epoch: 4, Steps: 118 | Train Loss: 0.1384103 Vali Loss: 0.1890018 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.190463 --> 0.189002).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.1014570\n",
      "\tspeed: 0.0094s/iter; left time: 105.8741s\n",
      "Epoch: 5 cost time: 0.6234986782073975\n",
      "Epoch: 5, Steps: 118 | Train Loss: 0.1359610 Vali Loss: 0.1896801 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.1033866\n",
      "\tspeed: 0.0089s/iter; left time: 99.0334s\n",
      "Epoch: 6 cost time: 0.6344151496887207\n",
      "Epoch: 6, Steps: 118 | Train Loss: 0.1350077 Vali Loss: 0.1855842 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.189002 --> 0.185584).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.1373771\n",
      "\tspeed: 0.0091s/iter; left time: 100.2500s\n",
      "Epoch: 7 cost time: 0.6488714218139648\n",
      "Epoch: 7, Steps: 118 | Train Loss: 0.1343328 Vali Loss: 0.1925521 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 1.5625e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.1441460\n",
      "\tspeed: 0.0093s/iter; left time: 101.1668s\n",
      "Epoch: 8 cost time: 0.6594374179840088\n",
      "Epoch: 8, Steps: 118 | Train Loss: 0.1344338 Vali Loss: 0.1867031 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 7.8125e-07\n",
      "\titers: 100, epoch: 9 | loss: 0.1411773\n",
      "\tspeed: 0.0091s/iter; left time: 97.7251s\n",
      "Epoch: 9 cost time: 0.631176233291626\n",
      "Epoch: 9, Steps: 118 | Train Loss: 0.1336674 Vali Loss: 0.1858702 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 3.90625e-07\n",
      "\titers: 100, epoch: 10 | loss: 0.0967780\n",
      "\tspeed: 0.0089s/iter; left time: 94.8251s\n",
      "Epoch: 10 cost time: 0.625054121017456\n",
      "Epoch: 10, Steps: 118 | Train Loss: 0.1342993 Vali Loss: 0.1829538 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.185584 --> 0.182954).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.1012642\n",
      "\tspeed: 0.0087s/iter; left time: 91.8974s\n",
      "Epoch: 11 cost time: 0.6234872341156006\n",
      "Epoch: 11, Steps: 118 | Train Loss: 0.1343934 Vali Loss: 0.1804598 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.182954 --> 0.180460).  Saving model ...\n",
      "Updating learning rate to 9.765625e-08\n",
      "\titers: 100, epoch: 12 | loss: 0.1514730\n",
      "\tspeed: 0.0089s/iter; left time: 92.7710s\n",
      "Epoch: 12 cost time: 0.6299309730529785\n",
      "Epoch: 12, Steps: 118 | Train Loss: 0.1340377 Vali Loss: 0.1776201 Test Loss: 0.0000000\n",
      "Validation loss decreased (0.180460 --> 0.177620).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-08\n",
      "\titers: 100, epoch: 13 | loss: 0.0827678\n",
      "\tspeed: 0.0091s/iter; left time: 93.9056s\n",
      "Epoch: 13 cost time: 0.6356973648071289\n",
      "Epoch: 13, Steps: 118 | Train Loss: 0.1343655 Vali Loss: 0.1902176 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 2.44140625e-08\n",
      "\titers: 100, epoch: 14 | loss: 0.1013007\n",
      "\tspeed: 0.0090s/iter; left time: 91.8624s\n",
      "Epoch: 14 cost time: 0.6388797760009766\n",
      "Epoch: 14, Steps: 118 | Train Loss: 0.1342789 Vali Loss: 0.1840130 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 1.220703125e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.1090477\n",
      "\tspeed: 0.0091s/iter; left time: 91.3348s\n",
      "Epoch: 15 cost time: 0.6351795196533203\n",
      "Epoch: 15, Steps: 118 | Train Loss: 0.1336045 Vali Loss: 0.1896696 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 6.103515625e-09\n",
      "\titers: 100, epoch: 16 | loss: 0.1211118\n",
      "\tspeed: 0.0090s/iter; left time: 89.7830s\n",
      "Epoch: 16 cost time: 0.6246898174285889\n",
      "Epoch: 16, Steps: 118 | Train Loss: 0.1343101 Vali Loss: 0.1896921 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "\titers: 100, epoch: 17 | loss: 0.1168757\n",
      "\tspeed: 0.0090s/iter; left time: 88.2093s\n",
      "Epoch: 17 cost time: 0.6434409618377686\n",
      "Epoch: 17, Steps: 118 | Train Loss: 0.1341794 Vali Loss: 0.1835847 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.1238319\n",
      "\tspeed: 0.0092s/iter; left time: 88.7225s\n",
      "Epoch: 18 cost time: 0.6307718753814697\n",
      "Epoch: 18, Steps: 118 | Train Loss: 0.1336638 Vali Loss: 0.1877962 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "\titers: 100, epoch: 19 | loss: 0.0935257\n",
      "\tspeed: 0.0089s/iter; left time: 85.2816s\n",
      "Epoch: 19 cost time: 0.6297810077667236\n",
      "Epoch: 19, Steps: 118 | Train Loss: 0.1342047 Vali Loss: 0.1851915 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Updating learning rate to 3.814697265625e-10\n",
      "\titers: 100, epoch: 20 | loss: 0.1780412\n",
      "\tspeed: 0.0088s/iter; left time: 83.2786s\n",
      "Epoch: 20 cost time: 0.6107203960418701\n",
      "Epoch: 20, Steps: 118 | Train Loss: 0.1342984 Vali Loss: 0.1814933 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Updating learning rate to 1.9073486328125e-10\n",
      "\titers: 100, epoch: 21 | loss: 0.0912734\n",
      "\tspeed: 0.0087s/iter; left time: 81.5924s\n",
      "Epoch: 21 cost time: 0.6155071258544922\n",
      "Epoch: 21, Steps: 118 | Train Loss: 0.1343097 Vali Loss: 0.1849295 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Updating learning rate to 9.5367431640625e-11\n",
      "\titers: 100, epoch: 22 | loss: 0.1366184\n",
      "\tspeed: 0.0090s/iter; left time: 82.6631s\n",
      "Epoch: 22 cost time: 0.617633581161499\n",
      "Epoch: 22, Steps: 118 | Train Loss: 0.1341458 Vali Loss: 0.1863259 Test Loss: 0.0000000\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.des, ii)\n",
    "\n",
    "model = experiment.train(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение предсказаний на настоящем test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seq_len = 12\n",
    "        self.label_len = 12\n",
    "        self.pred_len = 2\n",
    "        \n",
    "        self.is_training = 1\n",
    "        self.model_id = 'rosstat'\n",
    "        self.model = 'xPatch'\n",
    "        self.data = 'custom_multi'\n",
    "        \n",
    "        self.root_path = data_dir\n",
    "        self.data_path = 'xPatch_test_data.csv'\n",
    "        \n",
    "        self.features = 'MS'\n",
    "        self.target = 'nominal_wage'\n",
    "        self.freq = 'M'\n",
    "        self.scale = False\n",
    "        self.timeenc = 0\n",
    "        self.train_only = True\n",
    "        \n",
    "        self.enc_in = 10\n",
    "        self.embed = 'timeF'\n",
    "        \n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        self.padding_patch = 'end'\n",
    "        \n",
    "        self.ma_type = 'ema'\n",
    "        self.alpha = 0.3\n",
    "        self.beta = 0.3\n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.train_epochs = 100\n",
    "        self.patience = 10\n",
    "        self.learning_rate = 3e-4\n",
    "        self.loss = 'mse'\n",
    "        self.lradj = 'type1'\n",
    "        self.use_amp = False\n",
    "        self.revin = 1\n",
    "        \n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "        self.gpu = 0\n",
    "        self.use_multi_gpu = False\n",
    "        self.devices = '0'\n",
    "        self.test_flop = False\n",
    "        \n",
    "        self.checkpoints = './checkpoints/'\n",
    "        self.des = 'test'\n",
    "        self.itr = 1\n",
    "        self.num_workers = 10\n",
    "\n",
    "        self.train_perc = 1.\n",
    "        self.test_perc = 0.5\n",
    "\n",
    "args = Args()\n",
    "assert not args.scale, 'Нормализацию нужно проводить вручную'\n",
    "experiment = Exp_Main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69, 11)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_path = 'xPatch_val_data.csv'\n",
    "experiment = Exp_Main(args)\n",
    "\n",
    "val_dataset, _ = experiment._get_data(flag='train')\n",
    "true_val_datasets = val_dataset.datasets\n",
    "len(true_val_datasets), len(true_val_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      "train 759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69, 11)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_path = 'xPatch_test_data.csv'\n",
    "experiment = Exp_Main(args)\n",
    "\n",
    "test_dataset, _ = experiment._get_data(flag='train')\n",
    "true_test_datasets = test_dataset.datasets\n",
    "len(true_test_datasets), len(true_test_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "val_predictions = {}\n",
    "val_predictions_df = pd.DataFrame()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "all_codes = train_df['code'].unique()\n",
    "\n",
    "for code, true_val_dataset in zip(all_codes, true_val_datasets):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    for i in tqdm(range(0, len(true_val_dataset), args.pred_len), disable=True):\n",
    "        batch_x, batch_y, _, _ = true_val_dataset[i]\n",
    "        batch_x = torch.tensor(batch_x).unsqueeze(0).float().to(device)\n",
    "        batch_y = torch.tensor(batch_y).unsqueeze(0).float().to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch_x)\n",
    "\n",
    "        y_pred = scalers['nominal_wage'].inverse_transform(preds.detach().squeeze(0).cpu().numpy())[..., -1:].flatten()\n",
    "        y_true = scalers['nominal_wage'].inverse_transform(batch_y.detach().squeeze(0).cpu().numpy())[:preds.shape[1], -1:].flatten()\n",
    "\n",
    "        all_preds.append(y_pred)\n",
    "        all_true.append(y_true)\n",
    "\n",
    "    predictions = np.concatenate(all_preds)\n",
    "    true_values = np.concatenate(all_true)\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "                    predictions,\n",
    "                    true_values,\n",
    "                ]).transpose()\n",
    "    \n",
    "    df.columns = ['mean', 'y_true']\n",
    "    df['code'] = code\n",
    "    df['0.1'] = get_quantile_from_median(df['mean'].values, 0.1)\n",
    "    df['0.9'] = get_quantile_from_median(df['mean'].values, 0.9)\n",
    "\n",
    "    val_predictions_df = pd.concat([val_predictions_df, df])\n",
    "\n",
    "val_predictions['xPatch'] = val_predictions_df.set_index('code')\n",
    "\n",
    "\n",
    "test_predictions = {}\n",
    "test_predictions_df = pd.DataFrame()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "all_codes = train_df['code'].unique()\n",
    "\n",
    "for code, true_test_dataset in zip(all_codes, true_test_datasets):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    for i in tqdm(range(0, len(true_test_dataset), args.pred_len), disable=True):\n",
    "        batch_x, batch_y, _, _ = true_test_dataset[i]\n",
    "        batch_x = torch.tensor(batch_x).unsqueeze(0).float().to(device)\n",
    "        batch_y = torch.tensor(batch_y).unsqueeze(0).float().to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(batch_x)\n",
    "\n",
    "        y_pred = scalers['nominal_wage'].inverse_transform(preds.detach().squeeze(0).cpu().numpy())[..., -1:].flatten()\n",
    "        y_true = scalers['nominal_wage'].inverse_transform(batch_y.detach().squeeze(0).cpu().numpy())[:preds.shape[1], -1:].flatten()\n",
    "\n",
    "        all_preds.append(y_pred)\n",
    "        all_true.append(y_true)\n",
    "\n",
    "    predictions = np.concatenate(all_preds)\n",
    "    true_values = np.concatenate(all_true)\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "                    predictions,\n",
    "                    true_values,\n",
    "                ]).transpose()\n",
    "    \n",
    "    df.columns = ['mean', 'y_true']\n",
    "    df['code'] = code\n",
    "    df['0.1'] = get_quantile_from_median(df['mean'].values, 0.1)\n",
    "    df['0.9'] = get_quantile_from_median(df['mean'].values, 0.9)\n",
    "\n",
    "    test_predictions_df = pd.concat([test_predictions_df, df])\n",
    "\n",
    "test_predictions['xPatch'] = test_predictions_df.set_index('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_forecasts_val_test(\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    val_predictions: pd.DataFrame,\n",
    "    test_predictions: dict[str, pd.DataFrame],\n",
    "    start_date: str | None = None,\n",
    "    end_date: str | None = None,\n",
    "    height: int = 300,\n",
    "    width: int = 1400,\n",
    "    item_id: str | None = None,\n",
    "):\n",
    "    model_names = list(test_predictions.keys())\n",
    "    n_models = len(model_names)\n",
    "\n",
    "    if n_models == 1:\n",
    "        rows, cols = 1, 1\n",
    "    else:\n",
    "        rows = int(np.ceil(n_models / 2))\n",
    "        cols = 2\n",
    "    \n",
    "    filtered_val_df = val_df.copy()\n",
    "    filtered_test_df = test_df.copy()\n",
    "    \n",
    "    if start_date is not None and end_date is not None:        \n",
    "        test_mask = (filtered_test_df[\"timestamp\"] >= start_date) & (filtered_test_df[\"timestamp\"] <= end_date)\n",
    "        filtered_test_df = filtered_test_df[test_mask]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=rows, cols=cols, subplot_titles=model_names, vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        row = i // cols + 1\n",
    "        col = i % cols + 1\n",
    "\n",
    "        model_val_pred = val_predictions[model_name].copy()\n",
    "        if item_id is not None:\n",
    "            model_val_pred = model_val_pred.loc[item_id]\n",
    "\n",
    "        filtered_val_mean = model_val_pred[\"mean\"].values\n",
    "        filtered_val_upper = model_val_pred[\"0.9\"].values if \"0.9\" in model_val_pred else None\n",
    "        filtered_val_lower = model_val_pred[\"0.1\"].values if \"0.1\" in model_val_pred else None\n",
    "        \n",
    "        model_test_pred = test_predictions[model_name]\n",
    "        if item_id is not None:\n",
    "            model_test_pred = model_test_pred.loc[item_id]\n",
    "        \n",
    "        if start_date is not None and end_date is not None:\n",
    "            test_time_mask = (test_df[\"timestamp\"] >= start_date) & (test_df[\"timestamp\"] <= end_date)\n",
    "            \n",
    "            filtered_test_mean = model_test_pred[\"mean\"].values[test_time_mask]\n",
    "            filtered_test_upper = model_test_pred[\"0.9\"].values[test_time_mask] if \"0.9\" in model_test_pred else None\n",
    "            filtered_test_lower = model_test_pred[\"0.1\"].values[test_time_mask] if \"0.1\" in model_test_pred else None\n",
    "            \n",
    "        else:\n",
    "            filtered_test_mean = model_test_pred[\"mean\"]\n",
    "            filtered_test_upper = model_test_pred[\"0.9\"] if \"0.9\" in model_test_pred else None\n",
    "            filtered_test_lower = model_test_pred[\"0.1\"] if \"0.1\" in model_test_pred else None\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=filtered_val_df[\"timestamp\"],\n",
    "                y=filtered_val_df[\"target\"],\n",
    "                name=\"Validation (actual)\",\n",
    "                line=dict(color=\"blue\"),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=filtered_val_df[\"timestamp\"],\n",
    "                y=filtered_val_mean,\n",
    "                name=\"Validation (predicted)\",\n",
    "                line=dict(color=\"purple\", dash=\"dot\"),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=filtered_test_df[\"timestamp\"],\n",
    "                y=filtered_test_df[\"target\"],\n",
    "                name=\"Test (actual)\",\n",
    "                line=dict(color=\"#50C878\"),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=filtered_test_df[\"timestamp\"],\n",
    "                y=filtered_test_mean,\n",
    "                name=f\"{model_name} Test (predicted)\",\n",
    "                line=dict(color=\"#D70040\", dash=\"dot\"),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=filtered_val_df[\"timestamp\"],\n",
    "                    y=filtered_val_upper,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=0),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=filtered_val_df[\"timestamp\"],\n",
    "                y=filtered_val_lower,\n",
    "                mode=\"lines\",\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=\"rgba(128, 0, 128, 0.6)\",\n",
    "                line=dict(width=0),\n",
    "                name=f\"{model_name} CI (0.1-0.9)\",\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "        \n",
    "        if filtered_test_upper is not None and filtered_test_lower is not None:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=filtered_test_df[\"timestamp\"],\n",
    "                    y=filtered_test_upper,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=0),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=filtered_test_df[\"timestamp\"],\n",
    "                    y=filtered_test_lower,\n",
    "                    mode=\"lines\",\n",
    "                    fill=\"tonexty\",\n",
    "                    fillcolor=\"rgba(255, 127, 14, 0.6)\",\n",
    "                    line=dict(width=0),\n",
    "                    name=f\"{model_name} CI (0.1-0.9)\",\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Forecasts with Validation and Test Data\",\n",
    "        template=\"plotly_white\",\n",
    "        height=height * rows,\n",
    "        width=width,\n",
    "        showlegend=True,\n",
    "    )\n",
    "    \n",
    "    for i in range(rows * cols):\n",
    "        row = i // cols + 1\n",
    "        col = i % cols + 1\n",
    "        \n",
    "        fig.update_xaxes(\n",
    "            title_text=\"Date\",\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            title_text=\"National Demand\",\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee31734daf46d1857d13d8a1973302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(DatePicker(value=datetime.date(2023, 1, 1), description='Start date:'), DatePick…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ec0a1632cd47f59dbdae77d10fc71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import datetime\n",
    "\n",
    "\n",
    "date_col = pd.to_datetime(test_df[\"date\"])\n",
    "min_date = date_col.min().date()\n",
    "max_date = date_col.max().date()\n",
    "height = 400\n",
    "width = 1200\n",
    "\n",
    "start_date_picker = widgets.DatePicker(\n",
    "    description=\"Start date:\", disabled=False, value=min_date\n",
    ")\n",
    "\n",
    "end_date_picker = widgets.DatePicker(\n",
    "    description=\"End date:\", disabled=False, value=max_date\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        start_date = datetime.datetime.combine(\n",
    "            start_date_picker.value, datetime.datetime.min.time()\n",
    "        )\n",
    "        end_date = datetime.datetime.combine(\n",
    "            end_date_picker.value, datetime.datetime.min.time()\n",
    "        )\n",
    "        plot_forecasts_val_test(\n",
    "            val_df=val_df_,\n",
    "            test_df=test_df_,\n",
    "            val_predictions=all_val_models_predictions_,\n",
    "            test_predictions=test_predictions,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            item_id=item_id,\n",
    "        )\n",
    "\n",
    "\n",
    "plot_button = widgets.Button(description=\"Plot Forecasts\")\n",
    "plot_button.on_click(on_button_clicked)\n",
    "\n",
    "controls = widgets.VBox(\n",
    "    [widgets.HBox([start_date_picker, end_date_picker]), plot_button]\n",
    ")\n",
    "\n",
    "display(controls, output_area)\n",
    "\n",
    "item_id = 5\n",
    "\n",
    "val_df_ = val_df.rename(columns={'date': 'timestamp', \"nominal_wage\": \"target\"})[['code', 'timestamp', \"target\"]]\n",
    "val_df_ = val_df_[val_df_['code'].eq(item_id)].reset_index(drop=True)\n",
    "val_df_['timestamp'] = pd.to_datetime(val_df_['timestamp'])\n",
    "\n",
    "test_df_ = test_df.rename(columns={'date': 'timestamp', \"nominal_wage\": \"target\"})[['code', 'timestamp', \"target\"]]\n",
    "test_df_ = test_df_[test_df_['code'].eq(item_id)].reset_index(drop=True)\n",
    "test_df_['timestamp'] = pd.to_datetime(test_df_['timestamp'])\n",
    "\n",
    "val_df_ = pd.concat([val_df_, test_df_.iloc[[0]]])\n",
    "\n",
    "all_val_models_predictions_ = val_predictions.copy()\n",
    "for model_ in all_val_models_predictions_.keys():\n",
    "    all_val_models_predictions_[model_] = pd.concat([all_val_models_predictions_[model_], test_predictions[model_].loc[[item_id]].iloc[[0]]])\n",
    "\n",
    "with output_area:\n",
    "    plot_forecasts_val_test(\n",
    "        val_df=val_df_,\n",
    "        test_df=test_df_,\n",
    "        val_predictions=all_val_models_predictions_,\n",
    "        test_predictions=test_predictions,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        item_id=item_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xPatch': {'MSE': 139931862.48488912,\n",
       "  'MAE': 6893.663443538645,\n",
       "  'MAPE': 7.7432307135237,\n",
       "  'MASE': 3.3260897016930726,\n",
       "  'SQL': 2433.326156542496}}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models_metrics = {}\n",
    "\n",
    "test_predictions\n",
    "for model in test_predictions.keys():\n",
    "    metrics_df = []\n",
    "    for code in all_codes:\n",
    "        pred_df = pd.concat([\n",
    "            test_predictions[model].rename(columns={'mean': '0.5'})\n",
    "            .loc[code][[\"0.1\", \"0.5\", \"0.9\"]]\n",
    "            .reset_index(drop=True),\n",
    "            test_df[test_df[\"code\"].eq(code)][[\"nominal_wage\"]].reset_index(drop=True),\n",
    "        ], axis=1)\n",
    "        pred_df = pd.DataFrame(pred_df)\n",
    "\n",
    "        metrics_df.append(calculate_sklearn_metrics(pred_df, target_column='nominal_wage'))\n",
    "\n",
    "    metrics_dict = pd.DataFrame(metrics_df).mean().to_dict()\n",
    "\n",
    "    all_models_metrics[model] = metrics_dict\n",
    "\n",
    "all_models_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run xPatch_xPatch at: http://127.0.0.1:5000/#/experiments/169882278836627198/runs/8336b198560a4747bf9a7f81c569cda8\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/169882278836627198\n"
     ]
    }
   ],
   "source": [
    "prefix = 'xPatch'\n",
    "\n",
    "for k, metrics_ in all_models_metrics.items():\n",
    "    run_name = f\"{k}_{prefix}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_metrics(metrics_)\n",
    "        mlflow.log_param(\"model_name\", k)\n",
    "\n",
    "        mlflow.set_tag(\"prefix\", prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
